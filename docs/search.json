[
  {
    "objectID": "module4.html",
    "href": "module4.html",
    "title": "Module 4",
    "section": "",
    "text": "The measure of overall (dis)similarity between two or more genomes helps in understanding relationships between different organisms.\nThis is often used for typing bacterial strains in epidemiological studies for species-level to strain-level (subspecies) identification.\nThis can also be used to quickly identify any outliers or unusual genomes in your dataset.\nGenomic distance estimation across your dataset will (or should!) be the first thing you do before beginning your analyses.\nThere are multiple approaches to go about this ranging from fast and approximate to slow and accurate.\n\n\n\nMLST is a method used to classify bacteria based on the sequences specific, highly conserved housekeeping genes.\nSince housekeeping genes are highly stable (less prone to random mutations as their functions are critical), this provides a stable and reproducible way of typing bacterial strains.\nFrom a pathogen surveillance perspective, it is useful for quickly identifying the bacterial strain that is responsible for an ongoing outbreak.\nThere are multiple MLST methods, the most common being 7-gene MLST. This method identifies 7 specific housekeeping gene alleles and assigns a type, referred to as the ST (Sequence Type), based on the combination of alleles present.\n\n\n\n\n7 Gene MLST\n\n\nThe specific 7 genes used for this typing differs across species. This is referred to as the MLST schema. Different organisms are typed according to different MLST schema. As we saw before, EnteroBase has MLST information for certain species. PubMLST is also a good resource to explore different MLST schema for different species.\n\n\nUse the mlst tool to identify the ST type for the sample genomes. They should be available in test_datasets/salmonella_assemblies/, or as we saw in the previous module you can download assemblies using datasets.\n\n\nClick to reveal answer\n\nmlst --nopath NWU_2025_workshop_data/test_datasets/salmonella_assemblies/*.fna &gt; mlst_table.tsv\nNWU_2025_workshop_data/test_datasets/salmonella_assemblies/*.fna : What does the * mean here?\n\nThe * signifies a wildcard.\nwildcards in bash are specific characters that have special meaning and they can help in finding files, directories or text that match patterns, as opposed to having to type out the full names every time.\n* means “anything” . *.fna means anything that ends with .fna. G*.fna means anything that begins with a G and ends with fna, and so on.\nThere are several other wildcards in bash with different special meanings.\nWildcards are powerful tools that can help you quickly find text, make specific manipulations to text files, and find files and folders in your computer by specifying patterns.\nLearn more about wildcards here!\n\n\nAbove we used the 7-Gene MLST method, however there are other MLST methods such as wgMLST and cgMLST which instead of only 7 genes, use alleles from the whole genome or the core genome respectively. These methods use more than just seven genes and therefore offer higher resolution in typing. chewBBACA is a popular software for creating and determining cgMLST/wgMLST.\nDepending on the organism, different typing schemes are widely used. Consult up-to-date literature for your organism of interest to see which is the most prevalent typing scheme in your case.\n\n\n\n\n\nMinHash or MASH is a method for estimating approximate genomic distances by comparing k-mers\nA given whole genome sequence is divided into several very short sequences called K-mers, where K stands for the length of the short sequences. (for example, if the genome was divided into several 31bp long sequences, K = 31)\nThese K-mers are then compared to see how many identical and how many non-identical k-mers there are between two different genomes.\nThe exact computation can be seen here but in effect this gives you an approximate similarity or dissimilarity score.\nThis method is very fast and computationally efficient, making it easy to compare thousands to tens of thousands of genomes in a very short time.\n\n\n\nUse the mashtree tool to calculate all-vs-all pairwise MASH distances for the genomes in test_datasets/salmonella_assemblies/. “all-vs-all pairwise” means we will be comparing every genome in our set to every other genome.\n\n\n\n\n\n\nNote\n\n\n\nmashtree by default only reports a dendrogram - which is a tree-like hierarchical grouping of similar items. We need this dendrogram for visualization purposes, but make sure to also output a table or matrix so that we have the actual distance values as well\n\n\n\n\nClick to reveal answer\n\nmashtree --numcpus 4 --outmatrix matrix.tsv --outtree tree.dnd NWU_2025_workshop_data/test_datasets/salmonella_assemblies/*.fna\n\n\n\n\n\n\nA method to calculate the genomic similarity by comparing the percentage of identical nucleotide sequences between two genomes.\nIt is similar to MASH in that the two genomes being compared are fragmented into shorter sequences but these fragments are much larger (~1000 bp) than the MASH k-mers.\nThe sequence identity of each fragment to its orthologous fragment (i.e the most similar fragment in the other genome) is measured and this is done for all orthologous fragments.\nThe average of this nucleotide identity is then reported.\nThis method is more accurate than MASH but is also more computationally intensive.\nANI is commonly used for species delineation but can also be used for sub-species delineation.\n\n\n\nUse the skani tool to calculate all-vs-all pairwise ANI for the genomes in test_datasets/salmonella_assemblies/.\nSpecifically you must use skani dist as skani has other functions that we dont need right now. Look at the help page by running skani dist --help\n\n\nClick to reveal answer\n\nskani dist -q NWU_2025_workshop_data/test_datasets/salmonella_assemblies/*.fna -r NWU_2025_workshop_data/test_datasets/salmonella_assemblies/*.fna -o skani.tsv\nSince we are performing an all-vs-all comparison, the list of query sequences and the list of reference sequences to be compared in our case are the same, hence -q and -r have the same arguments. However with skani (and mash) you can compare two mutually exclusive lists as well, or compare a list with a single genome, depending on your needs.\n\nTake a look at the ANI values reported by skani in the output for each pair of genomes. How do they compare with the mash distances for the same pair?\n\n\n\n\n\n\nTipCleaning up output files\n\n\n\n\n\nYou may have noticed that the skani output has the full file paths for each sample (if you specified it in the input). In our final output, this is actually a problem as we cannot match the IDs with the IDs from other results due to the paths being appended. As we learned before, we can use sed to find and replace text, but in this case we have a complex long pattern that changes with each line. In these cases, we can use a powerful concept called regular expressions. Regular expressions or regex are a way to describe complex patterns as opposed to exact text. The regex to remove the paths from the skani output would be:\nsed 's/NWU_.*\\/\\(GCA_.*\\)_genomic.fna\\tNWU_.*\\/\\(GCA_.*\\)_genomic.fna/\\1\\t\\2/g' skani.tsv &gt; skani_cleaned.tsv\nRegular expressions are a complex topic that is beyond the scope of this workshop, but nevertheless it is an essential tool in the command line. You can learn more about regex here.\n\n\n\n\n\n\n\nWe learned about SNP (Single Nucleotide Polymorphism) calling in Module2. When performing SNP calling, we align a genome to a reference and identify specific nucleotide differences between the genome and the reference. The exact number of these nucleotide differences is called the SNP distance. This SNP calling can be performed for several genomes, all aligned against the same reference.\nand the all-vs-all pairwise number of SNPs can be estimated relative to that reference.\nThis is the most accurate and the most time consuming way to compare two genomes, however the resolution provided is significantly greater than ANI or MASH. For example, when comparing two genomes that are less than 10 SNPs apart (as is often the case during outbreaks), ANI or MASH will not be able to capture those differences.\nHowever, there are some key factors that need to be considered before calculating SNP distances.\n\n\nThe reference genome will act as an anchor or the starting point for the alignmnent tool. By definition, sequence alignments will be biased to the reference genome, therefore the choice of reference can greatly impact the quality of the alignment. Ideally, we would want a high quality, complete genome closely related to our samples to act as the reference. This is where the genomic comparison methods we used above can be useful.\n\n\nIf all your samples belong to the same ST, you can choose an appropriate high-quality reference that is also of the same ST.\n\n\n\nIf an appropriate high-quality reference genome is not available, an alternative is to choose one of your own samples as the reference. This can be done by performing all-vs-all ANI or MASH distance estimation, and picking a reference genome that is approximately equidistant from all the remaining genomes (i.e the midpoint or centroid).\n\n\n\nIn some cases, your dataset might have two distinct lineages (subspecies/MLST types). In these cases, it is advisable perform your analyses separately with an appropriate reference for each group.\n\n\n\nIf your dataset is highly diverse with mutliple potential lineages, you can perform reference-free SNP calling. For example, the k-mer based approach, as employed by SNP calling pipeline kSNP, involves breaking up genomic data into odd-length k-mers, and then comparing the k-mers to each other, allowing the center nucleotide to vary.\nOther popular methods utilize pan-genome pipelines (e.g., Panaroo, PIRATE, roary) to identify and align orthologous gene clusters within a set of genomes; the resulting multiple sequence alignments can then be concatenated and queried for SNPs.\nOverall, reference-free approaches are valuable in situations where bacterial genomes are distant enough that no reference genome is adequately “close” to all the the genomes in the set.\n\n\n\n\nSNPs which cluster closely together may have been acquired via recombination and/or horizontal gene transfer (HGT), not independent mutations.\nFrom a SNP calling perspective, this will appear as if a large number of SNPs have occurred one after the other in a particular region of the genome, inflating the SNP distance, when it is really a single event that led to a significant change in the genome.\nWhen your sample set comprises of a set of diverse genomes, even if they are all the same ST (i.e your sample set comprises all the diversity within that ST, and not a group of almost identical clones), you must perform recombinaion/HGT masking.\n\n\n\nGenome alignment approaches can be categorized as either core-genome alignment or whole-genome alignment based. Core-genome alignment based approaches involve identifying orthologous sequences present in all genomes in a set, while whole-genome alignment approaches extend alignment to regions not present in all genomes.\nIn most cases, core-genome alignment is the correct approach. For highly clonal samples whole genome alignments can be performed, though in effect this would be equivalent to the core genome alignment.\nIf your sample set comprises extremely diverse set (eg: multiple species, or several distantly related STs), it is better to perform a concatenated core-gene alignment as done by the pangenome approaches (e.g., Panaroo, PIRATE)\nThis approach identifies core genes - genes present in all (or most) of your samples, and takes these core gene sequences for each sample and concatenates them together in the same order. This creates an artificial core genome for each of your samples and these core genomes are aligned. Pangenomes will be discussed in more detail in a later module.\n\n\nTo calculate SNP distances, we will first perform SNP calling using snippy and obtain a recombination-free core genome alignment using gubbins, filter the alignment so that it contains only variable sites using snp-sites, and then calculate SNP distances using snp-dists.\n\n\nTo run Snippy using multiple genomes as input (i.e., using the snippy-multi command), we need to provide Snippy with a tab-separated input file. This input file will have one line per each of our input genomes, with the name of the sample as the first column and the absolute path to the sample as the second column.\nfor filename in $(ls NWU_2025_workshop_data/test_datasets/salmonella_assemblies/*); do bname=$(basename $filename _genomic.fna); echo -e \"${bname}\\t$(pwd)/${filename}\"; done; &gt; snippy_input.tsv\n\n\n\nNow that we have our input file, we can use snippy-multi to create a script, which will tell Snippy which commands to run. To do this, run the following command:\nsnippy-multi snippy_input.tsv --ref NWU_2025_workshop_data/test_datasets/reference_genome/genomic.gbff --cpus 1 &gt; runme.sh\n\nsnippy-multi - Snippy’s command for dealing with multiple genomes as input\nsnippy_input.tsv - Provide snippy-multi with the path to our Snippy input file\n--ref - Provide snippy-multi with the path to our reference in .gbff format\n--cpus 1 Number of CPUs to use\n&gt; runme.sh Use stdout redirection with &gt; to direct the output of snippy-multi into a script called runme.sh\n\nOnce snippy-multi finishes, we should see a new file, runme.sh, in our current directory:\ncat runme.sh\nTo run Snippy, run the script you just created:\nsh ./runme.sh\nThis will take some time. Once your Snippy script finishes, you can type ls core.* to see the new files Snippy created:\nls core.*\nWe will be using several of these output files in subsequent steps.\nIn addition to these output files, Snippy also creates an output directory for each isolate in our data set. We won’t be using these. Let’s create a directory named snippy_final with only the files we are interested in for now:\nmkdir snippy_final\n\nOnce we’ve created our snippy_final directory, let’s move all of the Snippy results that we want to keep to snippy_final (i.e., all the files starting with core.*):\n\nmv core.* snippy_final\nIf we type ls snippy_final, we should see several files within snippy_final, each starting with core.:\nls snippy_final\nFor the rest of the lesson, let’s move to our snippy_final directory:\ncd snippy_final\nIf we type pwd, we should see that we are in our snippy_final directory:\npwd\nIf we type ls, we should see our Snippy output files:\nls\n\n\n\nThe file named core.full.aln is an output file produced by Snippy, which contains our genomic alignment. Specifically, core.full.aln file is a FASTA-formatted multiple sequence alignment file. It has one sequence for the reference, and one for each sample; each sequence has the same length as the reference sequence.\nTo show the names of the sequences in our genomic alignment, run the following command, which prints the sequence headers in our genomic alignment multi-FASTA:\ngrep \"&gt;\" core.full.aln \nWe should see all of our input genomes, plus our reference genome (Reference).\nWe want to supply our genomic alignment to Gubbins so that we can identify and remove recombination. However, core.full.aln contains some weird characters, which Gubbins (and other downstream programs) may not like. Before we can do anything else with core.full.aln, we need to “clean” it using Snippy’s snippy-clean_full_aln function. This will replace all the “weird” characters with N, so that we can use our genomic alignment as input to other tools.\nTo clean core.full.aln, let’s run snippy-clean_full_aln and output our clean alignment to a file named clean.full.aln:\nsnippy-clean_full_aln core.full.aln &gt; clean.full.aln\nOnce snippy-clean_full_aln finishes, we can type ls and see that a new file, clean.full.aln has been created:\nls\nOur new file, clean.full.aln, contains our clean genomic alignment. We can see that it contains the same sequence headers as core.full.aln:\ngrep \"&gt;\" clean.full.aln\n\n\n\nNow that we have our clean genomic alignment (clean.full.aln), we can use Gubbins to detect and remove recombination.\nrun_gubbins.py --help\nLet’s use Gubbins to detect and remove recombination in our clean genomic alignment, clean.full.aln; to do this, run the following command:\nrun_gubbins.py --prefix gubbins --threads 1 clean.full.aln\n\nrun_gubbins.py : Call Gubbins\n--prefix gubbins: Use gubbins as a prefix for the final output filenames\n--threads 1 : Number of threads/CPUs to use\nclean.full.aln : Path to our clean genomic alignment, which we will be used as input\n\nYou should see several new files produced with the prefix gubbins.*. You can read all about the output files produced by Gubbins here, in the Gubbins manual.\nWe will be using the file gubbins.filtered_polymorphic_sites.fasta in subsequent steps; this file is a multi-FASTA alignment of recombination-filtered SNPs (i.e., it is an alignment of SNPs alone and does not include constant sites).\n\n\n\nNow that we have an alignment of recombination-free SNPs present in our genomes, let’s filter that alignment to get only the core SNPs. To do this, we will use SNP-sites.\nUse SNP-sites to construct a final, recombination-free alignment of core SNPs\nsnp-sites -c gubbins.filtered_polymorphic_sites.fasta &gt; clean.core.fasta\n\nsnp-sites: Call SNP-sites\n-c : An option that tells snp-sites to only output columns containing exclusively ACGT (i.e., core SNPs; )\ngubbins.filtered_polymorphic_sites.fasta: Ppath to the recombination-free alignment of SNPs produced via gubbins as input\n&gt; clean.core.fasta: Write our final, recombination-free alignment of core SNPs to a file named clean.core.fasta\n\n\n\n\nsnp-dists -m clean.core.fasta &gt; clean.core.snp-dists.tsv\n\nsnp-dists: Call SNP-dists\n-m : An option to produce the output in “molten” format. Run the command without this option to see the difference!\n&gt; clean.core.snp-dists.tsv: Write our SNP distances output to a file called clean.core.snp-dists.tsv\n\n\nGenomic distance estimation methods have different strengths and applications, and there is no one-size-fits-all approach. While MLST is more for typing strains based on predefined genes, genome distance methods like MASH and ANI can provide broader, more comprehensive comparisons across whole genomes."
  },
  {
    "objectID": "module4.html#multi-locus-sequence-typing-mlst",
    "href": "module4.html#multi-locus-sequence-typing-mlst",
    "title": "Module 4",
    "section": "",
    "text": "MLST is a method used to classify bacteria based on the sequences specific, highly conserved housekeeping genes.\nSince housekeeping genes are highly stable (less prone to random mutations as their functions are critical), this provides a stable and reproducible way of typing bacterial strains.\nFrom a pathogen surveillance perspective, it is useful for quickly identifying the bacterial strain that is responsible for an ongoing outbreak.\nThere are multiple MLST methods, the most common being 7-gene MLST. This method identifies 7 specific housekeeping gene alleles and assigns a type, referred to as the ST (Sequence Type), based on the combination of alleles present.\n\n\n\n\n7 Gene MLST\n\n\nThe specific 7 genes used for this typing differs across species. This is referred to as the MLST schema. Different organisms are typed according to different MLST schema. As we saw before, EnteroBase has MLST information for certain species. PubMLST is also a good resource to explore different MLST schema for different species.\n\n\nUse the mlst tool to identify the ST type for the sample genomes. They should be available in test_datasets/salmonella_assemblies/, or as we saw in the previous module you can download assemblies using datasets.\n\n\nClick to reveal answer\n\nmlst --nopath NWU_2025_workshop_data/test_datasets/salmonella_assemblies/*.fna &gt; mlst_table.tsv\nNWU_2025_workshop_data/test_datasets/salmonella_assemblies/*.fna : What does the * mean here?\n\nThe * signifies a wildcard.\nwildcards in bash are specific characters that have special meaning and they can help in finding files, directories or text that match patterns, as opposed to having to type out the full names every time.\n* means “anything” . *.fna means anything that ends with .fna. G*.fna means anything that begins with a G and ends with fna, and so on.\nThere are several other wildcards in bash with different special meanings.\nWildcards are powerful tools that can help you quickly find text, make specific manipulations to text files, and find files and folders in your computer by specifying patterns.\nLearn more about wildcards here!\n\n\nAbove we used the 7-Gene MLST method, however there are other MLST methods such as wgMLST and cgMLST which instead of only 7 genes, use alleles from the whole genome or the core genome respectively. These methods use more than just seven genes and therefore offer higher resolution in typing. chewBBACA is a popular software for creating and determining cgMLST/wgMLST.\nDepending on the organism, different typing schemes are widely used. Consult up-to-date literature for your organism of interest to see which is the most prevalent typing scheme in your case."
  },
  {
    "objectID": "module4.html#minhash-distance-estimation",
    "href": "module4.html#minhash-distance-estimation",
    "title": "Module 4",
    "section": "",
    "text": "MinHash or MASH is a method for estimating approximate genomic distances by comparing k-mers\nA given whole genome sequence is divided into several very short sequences called K-mers, where K stands for the length of the short sequences. (for example, if the genome was divided into several 31bp long sequences, K = 31)\nThese K-mers are then compared to see how many identical and how many non-identical k-mers there are between two different genomes.\nThe exact computation can be seen here but in effect this gives you an approximate similarity or dissimilarity score.\nThis method is very fast and computationally efficient, making it easy to compare thousands to tens of thousands of genomes in a very short time.\n\n\n\nUse the mashtree tool to calculate all-vs-all pairwise MASH distances for the genomes in test_datasets/salmonella_assemblies/. “all-vs-all pairwise” means we will be comparing every genome in our set to every other genome.\n\n\n\n\n\n\nNote\n\n\n\nmashtree by default only reports a dendrogram - which is a tree-like hierarchical grouping of similar items. We need this dendrogram for visualization purposes, but make sure to also output a table or matrix so that we have the actual distance values as well\n\n\n\n\nClick to reveal answer\n\nmashtree --numcpus 4 --outmatrix matrix.tsv --outtree tree.dnd NWU_2025_workshop_data/test_datasets/salmonella_assemblies/*.fna"
  },
  {
    "objectID": "module4.html#ani-average-nucleotide-identity",
    "href": "module4.html#ani-average-nucleotide-identity",
    "title": "Module 4",
    "section": "",
    "text": "A method to calculate the genomic similarity by comparing the percentage of identical nucleotide sequences between two genomes.\nIt is similar to MASH in that the two genomes being compared are fragmented into shorter sequences but these fragments are much larger (~1000 bp) than the MASH k-mers.\nThe sequence identity of each fragment to its orthologous fragment (i.e the most similar fragment in the other genome) is measured and this is done for all orthologous fragments.\nThe average of this nucleotide identity is then reported.\nThis method is more accurate than MASH but is also more computationally intensive.\nANI is commonly used for species delineation but can also be used for sub-species delineation.\n\n\n\nUse the skani tool to calculate all-vs-all pairwise ANI for the genomes in test_datasets/salmonella_assemblies/.\nSpecifically you must use skani dist as skani has other functions that we dont need right now. Look at the help page by running skani dist --help\n\n\nClick to reveal answer\n\nskani dist -q NWU_2025_workshop_data/test_datasets/salmonella_assemblies/*.fna -r NWU_2025_workshop_data/test_datasets/salmonella_assemblies/*.fna -o skani.tsv\nSince we are performing an all-vs-all comparison, the list of query sequences and the list of reference sequences to be compared in our case are the same, hence -q and -r have the same arguments. However with skani (and mash) you can compare two mutually exclusive lists as well, or compare a list with a single genome, depending on your needs.\n\nTake a look at the ANI values reported by skani in the output for each pair of genomes. How do they compare with the mash distances for the same pair?\n\n\n\n\n\n\nTipCleaning up output files\n\n\n\n\n\nYou may have noticed that the skani output has the full file paths for each sample (if you specified it in the input). In our final output, this is actually a problem as we cannot match the IDs with the IDs from other results due to the paths being appended. As we learned before, we can use sed to find and replace text, but in this case we have a complex long pattern that changes with each line. In these cases, we can use a powerful concept called regular expressions. Regular expressions or regex are a way to describe complex patterns as opposed to exact text. The regex to remove the paths from the skani output would be:\nsed 's/NWU_.*\\/\\(GCA_.*\\)_genomic.fna\\tNWU_.*\\/\\(GCA_.*\\)_genomic.fna/\\1\\t\\2/g' skani.tsv &gt; skani_cleaned.tsv\nRegular expressions are a complex topic that is beyond the scope of this workshop, but nevertheless it is an essential tool in the command line. You can learn more about regex here."
  },
  {
    "objectID": "module4.html#snp-distances",
    "href": "module4.html#snp-distances",
    "title": "Module 4",
    "section": "",
    "text": "We learned about SNP (Single Nucleotide Polymorphism) calling in Module2. When performing SNP calling, we align a genome to a reference and identify specific nucleotide differences between the genome and the reference. The exact number of these nucleotide differences is called the SNP distance. This SNP calling can be performed for several genomes, all aligned against the same reference.\nand the all-vs-all pairwise number of SNPs can be estimated relative to that reference.\nThis is the most accurate and the most time consuming way to compare two genomes, however the resolution provided is significantly greater than ANI or MASH. For example, when comparing two genomes that are less than 10 SNPs apart (as is often the case during outbreaks), ANI or MASH will not be able to capture those differences.\nHowever, there are some key factors that need to be considered before calculating SNP distances.\n\n\nThe reference genome will act as an anchor or the starting point for the alignmnent tool. By definition, sequence alignments will be biased to the reference genome, therefore the choice of reference can greatly impact the quality of the alignment. Ideally, we would want a high quality, complete genome closely related to our samples to act as the reference. This is where the genomic comparison methods we used above can be useful.\n\n\nIf all your samples belong to the same ST, you can choose an appropriate high-quality reference that is also of the same ST.\n\n\n\nIf an appropriate high-quality reference genome is not available, an alternative is to choose one of your own samples as the reference. This can be done by performing all-vs-all ANI or MASH distance estimation, and picking a reference genome that is approximately equidistant from all the remaining genomes (i.e the midpoint or centroid).\n\n\n\nIn some cases, your dataset might have two distinct lineages (subspecies/MLST types). In these cases, it is advisable perform your analyses separately with an appropriate reference for each group.\n\n\n\nIf your dataset is highly diverse with mutliple potential lineages, you can perform reference-free SNP calling. For example, the k-mer based approach, as employed by SNP calling pipeline kSNP, involves breaking up genomic data into odd-length k-mers, and then comparing the k-mers to each other, allowing the center nucleotide to vary.\nOther popular methods utilize pan-genome pipelines (e.g., Panaroo, PIRATE, roary) to identify and align orthologous gene clusters within a set of genomes; the resulting multiple sequence alignments can then be concatenated and queried for SNPs.\nOverall, reference-free approaches are valuable in situations where bacterial genomes are distant enough that no reference genome is adequately “close” to all the the genomes in the set.\n\n\n\n\nSNPs which cluster closely together may have been acquired via recombination and/or horizontal gene transfer (HGT), not independent mutations.\nFrom a SNP calling perspective, this will appear as if a large number of SNPs have occurred one after the other in a particular region of the genome, inflating the SNP distance, when it is really a single event that led to a significant change in the genome.\nWhen your sample set comprises of a set of diverse genomes, even if they are all the same ST (i.e your sample set comprises all the diversity within that ST, and not a group of almost identical clones), you must perform recombinaion/HGT masking.\n\n\n\nGenome alignment approaches can be categorized as either core-genome alignment or whole-genome alignment based. Core-genome alignment based approaches involve identifying orthologous sequences present in all genomes in a set, while whole-genome alignment approaches extend alignment to regions not present in all genomes.\nIn most cases, core-genome alignment is the correct approach. For highly clonal samples whole genome alignments can be performed, though in effect this would be equivalent to the core genome alignment.\nIf your sample set comprises extremely diverse set (eg: multiple species, or several distantly related STs), it is better to perform a concatenated core-gene alignment as done by the pangenome approaches (e.g., Panaroo, PIRATE)\nThis approach identifies core genes - genes present in all (or most) of your samples, and takes these core gene sequences for each sample and concatenates them together in the same order. This creates an artificial core genome for each of your samples and these core genomes are aligned. Pangenomes will be discussed in more detail in a later module.\n\n\nTo calculate SNP distances, we will first perform SNP calling using snippy and obtain a recombination-free core genome alignment using gubbins, filter the alignment so that it contains only variable sites using snp-sites, and then calculate SNP distances using snp-dists.\n\n\nTo run Snippy using multiple genomes as input (i.e., using the snippy-multi command), we need to provide Snippy with a tab-separated input file. This input file will have one line per each of our input genomes, with the name of the sample as the first column and the absolute path to the sample as the second column.\nfor filename in $(ls NWU_2025_workshop_data/test_datasets/salmonella_assemblies/*); do bname=$(basename $filename _genomic.fna); echo -e \"${bname}\\t$(pwd)/${filename}\"; done; &gt; snippy_input.tsv\n\n\n\nNow that we have our input file, we can use snippy-multi to create a script, which will tell Snippy which commands to run. To do this, run the following command:\nsnippy-multi snippy_input.tsv --ref NWU_2025_workshop_data/test_datasets/reference_genome/genomic.gbff --cpus 1 &gt; runme.sh\n\nsnippy-multi - Snippy’s command for dealing with multiple genomes as input\nsnippy_input.tsv - Provide snippy-multi with the path to our Snippy input file\n--ref - Provide snippy-multi with the path to our reference in .gbff format\n--cpus 1 Number of CPUs to use\n&gt; runme.sh Use stdout redirection with &gt; to direct the output of snippy-multi into a script called runme.sh\n\nOnce snippy-multi finishes, we should see a new file, runme.sh, in our current directory:\ncat runme.sh\nTo run Snippy, run the script you just created:\nsh ./runme.sh\nThis will take some time. Once your Snippy script finishes, you can type ls core.* to see the new files Snippy created:\nls core.*\nWe will be using several of these output files in subsequent steps.\nIn addition to these output files, Snippy also creates an output directory for each isolate in our data set. We won’t be using these. Let’s create a directory named snippy_final with only the files we are interested in for now:\nmkdir snippy_final\n\nOnce we’ve created our snippy_final directory, let’s move all of the Snippy results that we want to keep to snippy_final (i.e., all the files starting with core.*):\n\nmv core.* snippy_final\nIf we type ls snippy_final, we should see several files within snippy_final, each starting with core.:\nls snippy_final\nFor the rest of the lesson, let’s move to our snippy_final directory:\ncd snippy_final\nIf we type pwd, we should see that we are in our snippy_final directory:\npwd\nIf we type ls, we should see our Snippy output files:\nls\n\n\n\nThe file named core.full.aln is an output file produced by Snippy, which contains our genomic alignment. Specifically, core.full.aln file is a FASTA-formatted multiple sequence alignment file. It has one sequence for the reference, and one for each sample; each sequence has the same length as the reference sequence.\nTo show the names of the sequences in our genomic alignment, run the following command, which prints the sequence headers in our genomic alignment multi-FASTA:\ngrep \"&gt;\" core.full.aln \nWe should see all of our input genomes, plus our reference genome (Reference).\nWe want to supply our genomic alignment to Gubbins so that we can identify and remove recombination. However, core.full.aln contains some weird characters, which Gubbins (and other downstream programs) may not like. Before we can do anything else with core.full.aln, we need to “clean” it using Snippy’s snippy-clean_full_aln function. This will replace all the “weird” characters with N, so that we can use our genomic alignment as input to other tools.\nTo clean core.full.aln, let’s run snippy-clean_full_aln and output our clean alignment to a file named clean.full.aln:\nsnippy-clean_full_aln core.full.aln &gt; clean.full.aln\nOnce snippy-clean_full_aln finishes, we can type ls and see that a new file, clean.full.aln has been created:\nls\nOur new file, clean.full.aln, contains our clean genomic alignment. We can see that it contains the same sequence headers as core.full.aln:\ngrep \"&gt;\" clean.full.aln\n\n\n\nNow that we have our clean genomic alignment (clean.full.aln), we can use Gubbins to detect and remove recombination.\nrun_gubbins.py --help\nLet’s use Gubbins to detect and remove recombination in our clean genomic alignment, clean.full.aln; to do this, run the following command:\nrun_gubbins.py --prefix gubbins --threads 1 clean.full.aln\n\nrun_gubbins.py : Call Gubbins\n--prefix gubbins: Use gubbins as a prefix for the final output filenames\n--threads 1 : Number of threads/CPUs to use\nclean.full.aln : Path to our clean genomic alignment, which we will be used as input\n\nYou should see several new files produced with the prefix gubbins.*. You can read all about the output files produced by Gubbins here, in the Gubbins manual.\nWe will be using the file gubbins.filtered_polymorphic_sites.fasta in subsequent steps; this file is a multi-FASTA alignment of recombination-filtered SNPs (i.e., it is an alignment of SNPs alone and does not include constant sites).\n\n\n\nNow that we have an alignment of recombination-free SNPs present in our genomes, let’s filter that alignment to get only the core SNPs. To do this, we will use SNP-sites.\nUse SNP-sites to construct a final, recombination-free alignment of core SNPs\nsnp-sites -c gubbins.filtered_polymorphic_sites.fasta &gt; clean.core.fasta\n\nsnp-sites: Call SNP-sites\n-c : An option that tells snp-sites to only output columns containing exclusively ACGT (i.e., core SNPs; )\ngubbins.filtered_polymorphic_sites.fasta: Ppath to the recombination-free alignment of SNPs produced via gubbins as input\n&gt; clean.core.fasta: Write our final, recombination-free alignment of core SNPs to a file named clean.core.fasta\n\n\n\n\nsnp-dists -m clean.core.fasta &gt; clean.core.snp-dists.tsv\n\nsnp-dists: Call SNP-dists\n-m : An option to produce the output in “molten” format. Run the command without this option to see the difference!\n&gt; clean.core.snp-dists.tsv: Write our SNP distances output to a file called clean.core.snp-dists.tsv\n\n\nGenomic distance estimation methods have different strengths and applications, and there is no one-size-fits-all approach. While MLST is more for typing strains based on predefined genes, genome distance methods like MASH and ANI can provide broader, more comprehensive comparisons across whole genomes."
  },
  {
    "objectID": "module2.html",
    "href": "module2.html",
    "title": "Module 2",
    "section": "",
    "text": "A very generalized genomics workflow would comprise the following -\n\n\n\nTypical genomics workflow\n\n\nWe will go through the reasoning for each of these steps and try out some common bioinformatics tools for each.\n\n\nDepending on the exact platform used, sequencing takes advantage of specific physical and/or chemical properties of different nucleotides. These physical/chemical properties are converted to signals that can be decoded by different algorithms to determine the DNA (or RNA) sequence. The exact workings of each sequencing platform is beyond the scope of this workshop. There are several great videos that explain the different sequencing technologies better than I ever can, I highly recommend checking them out if you are interested.\n\n\n\n\nFor most microbial sequencing purposes, Illumina short-read sequencing is the most common, especially for large number of samples\nHowever, due to the decreasing costs, long-read sequencing, especially from ONT (Oxford Nanopore Technologies) is becoming more common.\nIn general, my recommendation is:\n\nIf you have a lot of samples (tens to hundreds) go for Illumina (cheapest per sample)\nIf you have a handful of high-value samples (eg: very rare organism, time-sensitive early outbreak scenarios), go for ONT but watch out for potential sequencing errors. (more expensive)\nIf you have one or two samples that are of critical importance (for example your entire research project is based on this one strain) - perform hybrid sequencing (most expensive). Hybrid seqencing involves performing both Illumina short-read and ONT/PacBio long-read and combining reads from both to get the highest quality assembly possible. However, the quality of long-read sequencing is increasing so rapidly that long-read sequencing alone may be enough to get a high-quality complete assembly.\n\n\nThis workshop is geared towards Illumina Paired-end sequencing data as those are the most common\n\n\n\n\n\n\n\n\nShort strings of sequences that represent a highly fragmented version of the original genome.\nRaw unprocessed output from the sequencer.\n\n\n\n\n\nOverlapping reads are stitched together to make “contiguous” strings of DNA.\nThe more overlapping reads that are stitched, the longer the contig(s) will be and the fewer contigs you will have.\nIn an ideal scenario, a single chromosome will be a single contig.\nA collection of contigs for a given genome will be your assembly\n\n\n\n\n\nThe percentage of your original genome that is covered by the reads\n100% coverage means for every nucleotide in the original genome, there is at least one read that represents it.\nFor smaller genomes (eg: microbial) the coverage will almost always be 100% so it is assumed by default.\n\n\n\n\n\nThe average number of reads a given nucleotide in the is represented in.\n30x depth means on average a any given nucleotide is present in 30 reads.\nSometimes you might see something like “30x coverage” in many microbial papers. This means 100% of the genome is represented at a depth of 30x (“coverage” is assumed to be 100%).\n\n\n\n\n\nMost genome assemblies you will be working with are “draft” assemblies, meaning it is not a complete representation of the original genome, the assembly comprises multiple contigs that have not been perfectly stitched together.\n\n\n\n\n\nIf you perform long-read/hybrid sequencing you will have enough information in your reads to assemble the complete genome as a single continuous string with no breaks or gaps. This is referred to as a ‘closed’ or ‘complete’ genome assembly.\nTypically, bacteria have a single circular chromosome (some bacteria have multiple) and optionally some plasmids. When an assembly is referred to as ‘closed’ or ‘complete’, it means each chromosome and each plasmid (if any) is represented by a single contig respectively.\n\n\n\n\nSequencing Terminology\n\n\n\n\n\n\n\nRaw reads are what you get directly out of the sequencer.\nRaw reads are in FASTQ format ( See file formats in Module 0 ).\nFor Illumina Paired-end sequencing, you get two FASTQ files - the forward (R1) and reverse (R2) reads.\n\nEach fragment of DNA is sequenced twice, one from each end (i.e forward and reverse, or R1 and R2).\n\nThe information that connects each read in R1 to its mate in R2 is kept intact in the FASTQ files.\n\nFor each read, the FASTQ files also have per-base quality scores\nThe purpose of quality scores are straightforward - you want to know if the basecalling (i.e assigning the correct nucleotide base at the correct position for each read fragment) is good.\nUsing the quality scores, you can filter for only high-quality reads, and/or “trim” off lower quality sections in each read before you move to the next steps.\n\n\n\n\nPer-base sequence quality - You dont want too many low quality base calls\nAdapter removal - Adapters are artificial short sequences ligated to your samples’s actual DNA to help with the sequencing process. These adapters must be removed as they are not biologically significant\nAverage per-read quality - On average you want to keep only high quality reads\nN content - During basecalling, N is assigned (instead of A,T,G,C) if the basecaller is uncertain. You want to know how many uncertain calls were made\nRead length distribution - Reads that are too short are uninformative. Each read is expected to be ~150bp for Illumina\n\n\n\n\nLets download some raw reads and see what they look like\n\nThe Sequence Read Archive (SRA) or European Nucleotide Archive (ENA) are repositories from where we can download raw sequence reads.\nLets download raw reads for the sample SRR32528508\n\nActivate your conda environment if you haven’t already\n\nconda activate NWU_2025_workshop\n\nUse the software fasterq-dump to download the specific sample\n\nfasterq-dump SRR32528508\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have performed the setup.sh step in Module0 , the files should be present in NWU_2025_workshop_data/test_datasets/fastq_download/\n\n\n\nUse the head command you learned before to examine the first few lines of the fastq file\nLets now remove potential low quality reads and perform adapter trimming using fastp\n\nFirst, lets check how to use fastp using the --help command.\nJust like the built in command line tools, pretty much every bioinformatic software you install will also have a help page with instructions on how to use the tool\nWhile the concept will largely be the same, the exact syntax for using each software will differ slightly.\nThat is why it is very important to read these instructions every time you attempt to use a tool\n\nfastp --help\nBased on the --help page, we can see that the essential requirements for running fastp are:\n\ni or --in1 : Paired-end read 1 (R1) file as input\nI or --in2 : Paired-end read 2 (R2) file as input\no or --out1 : Name of quality-filtered paired-end read 1 (R1) output file\nO or --out2 : Name of quality-filtered paired-end read 2 (R2) output file\n\nfastp will also generate a QC report, by default the files will be called fastp.html and fastp.json, but we can rename those files using the --html and --json parameters.\nThere are many other options, feel free to go through them, but for our purposes there is no need to change anything else.\nNow we can run fastp on our sample\nfastp \\\n--in1 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_1.fastq \\\n--in2 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_2.fastq \\\n--out1 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_1_trimmed.fastq \\\n--out2 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_2_trimmed.fastq \\\n--html NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_trimmed.html \\\n--json NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_trimmed.json\nIn the above command, we gave the PATH to our our input reads to fastp through the in1 and in2 parameters, specified the names of the output files and the PATH in which they should be saved through the out1 and out2 parameters, along with the QC reports using --html and --json\nOpen the .html output in your browser and take a look at the results.\n\n\n\n\n\n\n\nTipWhy is there a \\ before every new parameter in the code?\n\n\n\n\n\nWhen you are writing commands on the command line, you can supply all the necessary arguments for whatever tool you are using continuously in one line (separated by spaces). For the sake of readability, I have split them into multiple lines. But when you simply enter a newline (i.e, press the enter key), the shell will assume you want to run your command and can lead to you running an incomplete command. Typing \\ before you press enter lets the shell know that you aren’t done yet and you are continuing with the same command.\n\n\n\n\n\n\n\n\n\n\nThe output from a sequencing machine contains large numbers of fragments (reads) of your original genome in no particular order.\nThese reads are quite short (~150bp with short read sequencing) - too short to obtain a lot of meaningful information as most genes are much longer than 150bp.\nThe same region of the genome will be repeated multiple times (depth)\nYour whole original genome might not be represented (coverage)\nAssembling your reads involves stitching together these fragments (in the right order) to create the closest possible representation of the original genome.\n\n\n\n\nBy Erekevan - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=116777958\n\n\n\n\n\n\n\n\nAdvantages:\n\nMore complete final assembly\nFewer contigs and longer contigs, meaning less fragmentation\n\nDisadvantages:\n\nThe assembled genome will be biased towards the reference.\nLarge insertions and rearrangements in your assembly will be lost as the reference might not have them (or have them at different locations).\n\nUse reference-based assembly if:\n\nYou think your genome is very closely related to an available reference\nYou made/evolved a mutant of your reference and want to confirm it\nYou know the genetic background of your genome and want to align it to an available reference of the same genetic background\nTools: bwa, bowtie2\n\n\n\n\n\n\nAdvantages:\n\nUnbiased assembly\nWill preserve “new” sequences in your genome that a typical reference might not have\n\nDisadvantages:\n\nMay be lower quality assemblies\nWill have many contigs and shorter contigs\nIt is very hard to resolve repetitive regions on your genome\n\nUse de novo assembly if:\n\nYou do not know the genetic background of your genome\nYou think your genome varies substantially from available reference genomes\nTools: SPAdes, SKESA\n\n\nIn most pathogen-genomics cases, de novo assemblies are the way to go\n\n\n\n\n\n\nTipShould you assemble your genome?\n\n\n\n\n\nIn most cases, yes. But remember that the assembly is a “consensus” of all the reads. So not all the information available in the reads will be represented in the final assembly. For example if you have low-abundance subpopulations in your samples, your assembly will only represent the sequence from the majority population.\n\n\n\n\n\n\n\n\nSimilar to fastp, lets look at the usage instructions for shovill\nshovill --help\nBased on the --help page, we can see that the essential requirements for running shovill are:\n\n--R1 : Paired-end read 1 (R1) file as input\n--R2 : Paired-end read 2 (R2) file as input\n--outdir : Path to the output directory that will be created to store outputs\n\nshovill is actually a software that comprises multiple other software. This type of tool is commonly referred to as a WRAPPER.\nIn this case, shovill is not the name of the actual assembly software, but it lets us choose between multiple assembly software.\nLet us choose skesa as the assembler. The default is another tool called spades but skesa is faster\n\n--assembler: Option to choose assembler, set to skesa\n\nGenome assembly tools may also use a lot of CPU and RAM, if you are running this on your personal computer, you many not have a lot of resources to spare, so it is a good idea to limit this using the --cpus and --ram parameters\nPutting it all together:\nshovill \\\n--R1 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_1_trimmed.fastq \\\n--R2 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_2_trimmed.fastq \\\n--outdir NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_skesa \\\n--assembler skesa \\\n--cpus 4 # Will use at max 4 CPU cores, Set according to your computer \\\n--ram 4 # Will use at max 4GB RAM, set according to your computer\n\n\n\n\n\n\n\nNote\n\n\n\nThis will take time, but you can use the complete assembly already available in NWU_2025_workshop_data/test_datasets/GCA_049744075.1/GCA_049744075.1_ASM4974407v1_genomic.fna for the next steps\n\n\n\nIn the above command, we gave the PATH to our QUALITY FILTERED reads  as input to shovill through the R1 and R2 parameters, specified the name of the output PATH using outdir, and set the assembler to skesa using --assembler skesa\nLook at the different output files using ls and head.\n\n\n\n\nSee how your assembly looks using quast - which is a software that outputs metrics on assembly quality. Take a look at the help page for quast and run it for your assembly!\n\n\nClick to reveal answer\n\nquast \\\n-o NWU_2025_workshop_data/test_datasets/GCA_049744075.1/quast \\\n-t 4 \\\nNWU_2025_workshop_data/test_datasets/GCA_049744075.1/GCA_049744075.1_ASM4974407v1_genomic.fna\n\n-o : Path to output directory\n-t : Number of threads to use\n&lt;contigs&gt; : Path to assembled contigs - This is a Positional argument\n\n\n\n\n\n\n\nTipPositional arguments\n\n\n\n\n\nYou may have noticed that the way you supplied the input files to quast is different from the way it was done for fastp or shovill. For some tools, you will explicitly state a specific parameter that accepts an input file. For example the --R1 parameter accepts the forward read file in shovill. However, with quast there is no such parameter for the input, it automatically considers the argument in the LAST position as the input. These are called POSITIONAL ARGUMENTS. Make sure to always check the --help page to see if the software you are using has positional arguments.\n\n\n\n\nTake a look at the reports generated by QUAST\n\n\n\n\nNumber of contigs - the fewer the better. For short-read assemblies, more than 100 is not great, more than 500 is bad.\nLargest contig size - the larger the better. You want your largest contig to represent as much of the original genome as possible, this means that a large portion of the genome is represented by one uninterrupted large contig.\nN50 value - the larger the better. This refers to the shortest contig such that the sum of the lengths of all contigs of that length or longer constitutes at least 50% of the total assembly length. In other words, imagine sorting all your contigs by length in descending order. How many contigs are required to reach 50% of the original genome’s length? The length of the final contig needed to reach 50% is the N50 value. This value will be higher if you have fewer contigs and longer contigs.\nNs per 100Kbp - the fewer the better. N will be called instead of an actual base (i.e A,T,G,C) if the basecaller is uncertain. You want as few uncertain base calls as possible.\n\n\n\n\n\n\nAnnotation is the process of identifying protein coding genes and other significant regions such as CRISPR sites or ncRNA\nGenome annotation can be divided into two steps:\n\nStructural annotation : Predicting Open Reading Frames (ORFs), coding regions, regulatory motifs, introns/exons etc.\nFunctional annotation : Attaching a potential biological function to protein-coding genes.\n\nHowever, from a workflow perspective, this is a single step process, especially for prokaryotes such as bacteria as the biological “rules” for what makes a gene is largely solved.\nMost annotation tools rely on a database of known gene sequences. The more similar a gene is in your sample to an existing, well characterized gene sequence, the more likely it is that the gene in your sample also shares the same functional properties.\nHowever, many annotation tools also rely on matching motifs or “patterns” of specific sequences rather than entire genes. These are helpful for annotating more diverged sequences that may not have an exact match in the reference databases.\nThere are several databases ranging from very broad to very specific collections of sequences that can aid in annotation.\n\nNCBI BLAST, DIAMOND: Searches input protein or NA sequence against available sequences of known function (maybe)\nVFDB, CARD, AMRFinderPlus : Identification of known virulence factors and antibiotic resistance genes\nSignalP, LipoP, tmHMM : Identification of signal peptides, lipoproteins and transmembrane proteins from conserved motifs\nKEGG, DAVID, PANTHER: Databases of gene/protein families and subfamilies used for tying biological pathways to genes\nNCBI CDART, SWISS-MODEL, PHYRE2, I-TASSER: Predicting functional domains and protein families from amino-acid sequence and secondary structure elements.\n\nCommon bacterial genome annotation software (such as prokka or bakta) combine multiple databases to provide the most complete annotations possible.\n\n\n\nUse prokka to annotate your assembly!\n\n\nClick to reveal answer\n\nprokka \\\n--outdir NWU_2025_workshop_data/test_datasets/GCA_049744075.1/prokka \\\n--cpu 4 \\\n--prefix GCA_049744075.1_ASM4974407v1 \\\nNWU_2025_workshop_data/test_datasets/GCA_049744075.1/GCA_049744075.1_ASM4974407v1_genomic.fna\n\n--output : Path to output directory\n--cpu : Number of CPUs to use\n--prefix : Output file “prefix” - this prefix will be added to the name of all outputs so that you can identify them\n&lt;contigs&gt; : Path to assembled contigs\n\n\nTake a look at the different output files prokka has generated and their corresponding file extensions!\n\n\n\n\n\n\nTipProkka vs Bakta - Use Bakta\n\n\n\n\n\nProkka used to be the industry standard bacterial genome annotation tool but it has not been updated in a long time. Prokka is still a good choice and is widely used, but Bakta is now the more up-to-date tool and it is fast becoming the standard. Prokka is faster and easier to run which is why we are using it for this workshop but for your research work you should use Bakta.\n\n\n\n\n\n\n\n\nSNP calling or variant calling is the process of identifying nucleotide differences amongst your samples\nCalling SNPs is critical for pathogen surveillance applications such as tracking the spread of strains responsible for outbreaks, identifying transmission events between hosts, and conducting genome-wide association studies (GWAS), in which SNPs are associated with phenotypes of interest (e.g., virulence, resistance to antimicrobials)\nThe most common approach to SNP calling relies on a reference genome, which is an assembled genome that is closely related to the genomes of interest.\nThis means and SNPs are identified relative to that reference. Therefore the choice of reference is critical for downstream analyses.\nIdeally, the reference genome used must be closely related to your sample collection, be of high quality (preferably a complete genome), and be associated with a publication ( or have good metadata) such that the reference strain has been characterized well.\n\n\n\n\nMapping reads to a reference genome GalaxyTraining Network, CC-BY-4.0\n\n\nYour input data can either be assemblies (.fasta) or post-QC reads (.fastq). SNP calling with assemblies will be faster but with reads will be more accurate. Different SNP calling tools can accept different inputs and you must choose one according to your needs and what type of data you have available.\nIn general, for bulk comparison of several isolates from a mix of sources where you are less interested in the actual functional effects SNPs and more about the overall similarity across your isolates, or if you need a core genome alignment, you can use assemblies. If you have a small collection of isolates (for eg: from an outbreak or from experimental evolution cultures) where calling accurate SNPs, as well as information regarding exact positions of SNPs and the potential mutations are important, you should use reads.\nWe will discuss different SNP calling pipelines, and other key considerations during SNP calling in a later module. For now since we are only working though a single example genome, let us run a basic SNP calling pipeline using snippy with a reference genome.\n\n\nUse snippy to identify variants in your assembly compared to the reference genome!\nIf you have run setup.sh from Module 0, you should have a folder called reference_genome inside test_datasets. Snippy can use either the genome assembly (_genomic.fna) or the Genbank annotation (genomic.gbff) as the reference.\nYou can also either provide the post-QC reads from fastp or the assembled contigs from shovill as input.\nCheck the snippy help page for how to perform each combination of input file(s) and reference file!\n\n\nClick to reveal answer\n\nsnippy \\\n--cpus 4 \\\n--ram 4 \\\n--outdir NWU_2025_workshop_data/test_datasets/GCA_049744075.1/snippy \\\n--reference NWU_2025_workshop_data/test_datasets/reference_genome/genomic.gbff \\\n--ctgs NWU_2025_workshop_data/test_datasets/GCA_049744075.1/GCA_049744075.1_ASM4974407v1_genomic.fna\n\n# OR\n--reference NWU_2025_workshop_data/test_datasets/reference_genome/GCA_000486855.2_ASM48685v2_genomic.fna \\\n--R1 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_1_trimmed.fastq \\\n--R2 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_2_trimmed.fastq\n\n--ram : The maximum amount of RAM to use\n--cpus : Number of CPUs to use\n--reference : Path to reference file in either .fasta or .gbff format\n--R1 & --R2: Path to post-QC reads\n\nOR\n\n--ctgs : Path to assembled contigs\n\n\nTake a look at the different output files snippy has generated and their corresponding file extensions! If you ran both read-based and assembly-based variant calling, take a look at the list of variants reported in each case. Are there differences? How about when you used a .fasta reference vs a .gbk ? Is the output any different?\n\n\n\n\n\nIn this module, we started with raw fastq reads from the sequencer, assembled the reads into a genome, identified genes in this genome, and compared this genome to a reference to identify SNPs. We performed these steps for one sample sequentially. However, what if you have multiple samples, all of which need to undergo the same steps?\nYes, you can simply copy-paste the above code, changing the input names accordingly for each sample you need to process. While this may work for a handful of samples, what if you have hundreds to process?\nThis is where scripting comes in. You can pre-write a set of instructions, and have those same instructions be repeated for all your samples.\n\n\nTake a look at this example bash script below. This is a script to perform genome annotation on a given assembly. Create a .txt file called prokka_script.sh and save the contents below into it (you can actually name it whatever you want, just make sure to give it the .sh extension so that you know it is a bash script).\n#!/bin/bash \n\nfpath=$1\n\nfname=$(basename ${fpath} _genomic.fna)\n\n# Running prokka\nprokka \\\n--outdir ${fname}_prokka \\\n--cpu 4 \\\n--prefix ${fname} \\\n--fast \\\n${fpath}\nLets go though this script line by line\n\n#!/bin/bash - this is called the shebang line. This line lets your computer know that this is a bash script.\n$fpath=$1\n\n$1 is a positional argument. It refers to the FIRST argument provided after calling the script. Similarly, $2 automatically refers to the second argument provided, $3 the third, and so on. But since prokka accepts needs only one input (the assembly), we only need $1 for this script.\nfpath is a new variable we are creating for the path to the input file. You can call this whatever you want, but just make sure to have variable names that make sense so that you know what it is referring to (calling variables x or y or var etc is bad practice!)\nfpath=$1 means fpath equals whatever is in $1. If we input the path to the fasta file that we need annotated as the first positional argument, fpath will now carry the path.\n\nfname=$(basename $fpath _genomic.fna)\n\nbasename is a bash command that takes the base name of the file that follows it. That means if you supply a filename with the absolute path, basename will keep only the actual name of the file, not the full path.\n${fpath} - we created a variable called fpath previously to hold the path to our input fasta file. Now we are calling the variable. When you want to call a variable in bash, you refer to the variable name after the $ sign enclosed in {}.\nbasename ${fpath} means the basename command will take the name of the input fasta file and not any file paths that may be associated with it.\nbasename ${fpath} _genomic.fna - we have now suppled _genomic.fna as an additonal argument to basename after ${fpath}. This tells basename the file suffix to also be removed. This may or may not be necessary depending on your input file. Since in our case all our files have the suffix _genomic.fna we can remove that too to only keep the actual name. Try it on the command line!\n\n\nbasename NWU_2025_workshop/NWU_2025_workshop_data/test_datasets/salmonella_assemblies/GCA_049744875.1_ASM4974487v1_genomic.fna _genomic.fna\n\nGCA_049744875.1_ASM4974487v1\n\nfname=$(basename ${fpath} _genomic.fna)\n\nWe enclose the basename command inside $() because we want to store the output of the command into a variable, and this variable will be called fname.\nThe reason we are storing the basename of our input file is because we want a way to control the name of the output file. Since we want to use this script for potentially any input fasta, we want to save the output file with an appropriate name that matches the input, so that we know which input file was used to generate which output. This is especially important when you are working with hundreds to thousands of samples as you always want to know exactly how each file was generated.\n\nprokka\n\nWhat follows is the same prokka script as before, but this time instead of directly specifying the name of the input fasta, we have specified the name of the variable.\nSimilarly, for the --outdir parameter, we have specified the basename of the input (${fname}) and appended _prokka to it, so that when we see the output directory we know it was created by prokka.\nWe are also providing ${fname} as the --prefix so that all files created by prokka will have the basename of the input file, again this is to match each output file with the corresponding input.\nFinally we provide ${fpath}, the actual input fasta file (with the absolute path) as the final argument for prokka (Remember that this is a positional argument for prokka so it has to be last)\n\n\nTo run this script on one sample, we can do\nchmod 755 ./prokka_script.sh # This makes the script executable, you only have to do this once\n\n./prokka_script.sh NWU_2025_workshop_data/test_datasets/salmonella_assemblies/GCA_049744875.1_ASM4974487v1_genomic.fna\n\nThis will run prokka for the sample GCA_049744875.1_ASM4974487v1_genomic.fna and save the output to a file called GCA_049744875.1_ASM4974487v1_prokka in the working directory.\n\n\n\n\nWe have still run it for only one sample, but now that we have scripted it, we can automate running this script for a list of samples. The simplest way to do this is using a for loop. This simply means we run the same set of instructions for each item in the list.\nFor loops in general follow this basic structure\nfor item in &lt;list of items&gt;\n  do \n  &lt;set of instructions&gt;\n  done\nThis means, for each item in a list/sequence, you can state a set of instructions to be performed, and it will end automatically when the last item in your list has been processed. The word item in the code above is a placeholder, it can be called anything you want, it is just a way to refer to a single item in the list of items.\nBelow is a simple bash for loop\nfor assembly in $(ls NWU_2025_workshop_data/test_datasets/salmonella_assemblies/); do\n  ./prokka_script.sh NWU_2025_workshop_data/test_datasets/salmonella_assemblies/${assembly}; \n  done\nExplanation:\n\nfor assembly in $(ls NWU_2025_workshop_data/test_datasets/salmonella_assemblies/); do\n\nls NWU_2025_workshop_data/test_datasets/salmonella_assemblies/ : Lists all the files in a folder. There are five assemblies provided in the test datasets. This is enclosed in $() because this is the  we want our for loop to go over.\nfor assembly in : assembly here is just what we are calling the variable. As the for loop goes over each item from the ls command, that item will be stored in a variable called assembly. As soon as one item is done being proccessed, the next item will now take its place in the assembly variable.\n; do : This is simply the bash for loop syntax. We are telling it to perform (or do) the following instructions\n\n./prokka_script.sh NWU_2025_workshop_data/test_datasets/salmonella_assemblies/${assembly} : Just like we ran the script before, we are calling prokka_script.sh and giving it as input whatever value is stored in ${assembly}, which in this case is the name of the assembled genome fasta file. We must provide the full path to the file so that prokka can correctly find it in the right location.\n; done : This again is part of for loop syntax. The instructions that need to be looped should be in-between do and done so that the loop knows when to start and when to stop.\n\n\n\namrfinder is a software used for annotating Anti-Microbial Resistance genes from a given genome.\nIf you performed the setup.sh step in Module0, you will have a set of 5 different assemblies in the folder NWU_2025_workshop_data/test_datasets/salmonella_assemblies/ . Write a loop to perform AMRFinderPlus on these five assemblies.\n\n\nClick to reveal answer\n\nfor i in $(ls NWU_2025_workshop_data/test_datasets/salmonella_assemblies/); do\nbname=$(basename NWU_2025_workshop_data/test_datasets/salmonella_assemblies/${i} _genomic.fna); \namrfinder --threads 1 --nucleotide NWU_2025_workshop_data/test_datasets/salmonella_assemblies/${i} --output ${bname}_amrfinderplus.tsv;\ndone"
  },
  {
    "objectID": "module2.html#genomics-workflow",
    "href": "module2.html#genomics-workflow",
    "title": "Module 2",
    "section": "",
    "text": "Short strings of sequences that represent a highly fragmented version of the original genome.\nRaw unprocessed output from the sequencer.\n\n\n\n\n\nOverlapping reads are stitched together to make “contiguous” strings of DNA.\nThe more overlapping reads that are stitched, the longer the contig(s) will be and the fewer contigs you will have.\nIn an ideal scenario, a single chromosome will be a single contig.\nA collection of contigs for a given genome will be your assembly\n\n\n\n\n\nThe percentage of your original genome that is covered by the reads\n100% coverage means for every nucleotide in the original genome, there is at least one read that represents it.\nFor smaller genomes (eg: microbial) the coverage will almost always be 100% so it is assumed by default.\n\n\n\n\n\nThe average number of reads a given nucleotide in the is represented in.\n30x depth means on average a any given nucleotide is present in 30 reads.\nSometimes you might see something like “30x coverage” in many microbial papers. This means 100% of the genome is represented at a depth of 30x (“coverage” is assumed to be 100%).\n\n\n\n\n\nMost genome assemblies you will be working with are “draft” assemblies, meaning it is not a complete representation of the original genome, the assembly comprises multiple contigs that have not been perfectly stitched together.\n\n\n\n\n\nIf you perform long-read/hybrid sequencing you will have enough information in your reads to assemble the complete genome as a single continuous string with no breaks or gaps. This is referred to as a ‘closed’ or ‘complete’ genome assembly.\nTypically, bacteria have a single circular chromosome (some bacteria have multiple) and optionally some plasmids. When an assembly is referred to as ‘closed’ or ‘complete’, it means each chromosome and each plasmid (if any) is represented by a single contig respectively.\n\n\n\n\nSequencing Terminology\n\n\n\n\n\n\n\nRaw reads are what you get directly out of the sequencer.\nRaw reads are in FASTQ format ( See file formats in Module 0 ).\nFor Illumina Paired-end sequencing, you get two FASTQ files - the forward (R1) and reverse (R2) reads.\n\nEach fragment of DNA is sequenced twice, one from each end (i.e forward and reverse, or R1 and R2).\n\nThe information that connects each read in R1 to its mate in R2 is kept intact in the FASTQ files.\n\nFor each read, the FASTQ files also have per-base quality scores\nThe purpose of quality scores are straightforward - you want to know if the basecalling (i.e assigning the correct nucleotide base at the correct position for each read fragment) is good.\nUsing the quality scores, you can filter for only high-quality reads, and/or “trim” off lower quality sections in each read before you move to the next steps.\n\n\n\n\nPer-base sequence quality - You dont want too many low quality base calls\nAdapter removal - Adapters are artificial short sequences ligated to your samples’s actual DNA to help with the sequencing process. These adapters must be removed as they are not biologically significant\nAverage per-read quality - On average you want to keep only high quality reads\nN content - During basecalling, N is assigned (instead of A,T,G,C) if the basecaller is uncertain. You want to know how many uncertain calls were made\nRead length distribution - Reads that are too short are uninformative. Each read is expected to be ~150bp for Illumina\n\n\n\n\nLets download some raw reads and see what they look like\n\nThe Sequence Read Archive (SRA) or European Nucleotide Archive (ENA) are repositories from where we can download raw sequence reads.\nLets download raw reads for the sample SRR32528508\n\nActivate your conda environment if you haven’t already\n\nconda activate NWU_2025_workshop\n\nUse the software fasterq-dump to download the specific sample\n\nfasterq-dump SRR32528508\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have performed the setup.sh step in Module0 , the files should be present in NWU_2025_workshop_data/test_datasets/fastq_download/\n\n\n\nUse the head command you learned before to examine the first few lines of the fastq file\nLets now remove potential low quality reads and perform adapter trimming using fastp\n\nFirst, lets check how to use fastp using the --help command.\nJust like the built in command line tools, pretty much every bioinformatic software you install will also have a help page with instructions on how to use the tool\nWhile the concept will largely be the same, the exact syntax for using each software will differ slightly.\nThat is why it is very important to read these instructions every time you attempt to use a tool\n\nfastp --help\nBased on the --help page, we can see that the essential requirements for running fastp are:\n\ni or --in1 : Paired-end read 1 (R1) file as input\nI or --in2 : Paired-end read 2 (R2) file as input\no or --out1 : Name of quality-filtered paired-end read 1 (R1) output file\nO or --out2 : Name of quality-filtered paired-end read 2 (R2) output file\n\nfastp will also generate a QC report, by default the files will be called fastp.html and fastp.json, but we can rename those files using the --html and --json parameters.\nThere are many other options, feel free to go through them, but for our purposes there is no need to change anything else.\nNow we can run fastp on our sample\nfastp \\\n--in1 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_1.fastq \\\n--in2 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_2.fastq \\\n--out1 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_1_trimmed.fastq \\\n--out2 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_2_trimmed.fastq \\\n--html NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_trimmed.html \\\n--json NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_trimmed.json\nIn the above command, we gave the PATH to our our input reads to fastp through the in1 and in2 parameters, specified the names of the output files and the PATH in which they should be saved through the out1 and out2 parameters, along with the QC reports using --html and --json\nOpen the .html output in your browser and take a look at the results.\n\n\n\n\n\n\n\nTipWhy is there a \\ before every new parameter in the code?\n\n\n\n\n\nWhen you are writing commands on the command line, you can supply all the necessary arguments for whatever tool you are using continuously in one line (separated by spaces). For the sake of readability, I have split them into multiple lines. But when you simply enter a newline (i.e, press the enter key), the shell will assume you want to run your command and can lead to you running an incomplete command. Typing \\ before you press enter lets the shell know that you aren’t done yet and you are continuing with the same command.\n\n\n\n\n\n\n\n\n\n\nThe output from a sequencing machine contains large numbers of fragments (reads) of your original genome in no particular order.\nThese reads are quite short (~150bp with short read sequencing) - too short to obtain a lot of meaningful information as most genes are much longer than 150bp.\nThe same region of the genome will be repeated multiple times (depth)\nYour whole original genome might not be represented (coverage)\nAssembling your reads involves stitching together these fragments (in the right order) to create the closest possible representation of the original genome.\n\n\n\n\nBy Erekevan - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=116777958\n\n\n\n\n\n\n\n\nAdvantages:\n\nMore complete final assembly\nFewer contigs and longer contigs, meaning less fragmentation\n\nDisadvantages:\n\nThe assembled genome will be biased towards the reference.\nLarge insertions and rearrangements in your assembly will be lost as the reference might not have them (or have them at different locations).\n\nUse reference-based assembly if:\n\nYou think your genome is very closely related to an available reference\nYou made/evolved a mutant of your reference and want to confirm it\nYou know the genetic background of your genome and want to align it to an available reference of the same genetic background\nTools: bwa, bowtie2\n\n\n\n\n\n\nAdvantages:\n\nUnbiased assembly\nWill preserve “new” sequences in your genome that a typical reference might not have\n\nDisadvantages:\n\nMay be lower quality assemblies\nWill have many contigs and shorter contigs\nIt is very hard to resolve repetitive regions on your genome\n\nUse de novo assembly if:\n\nYou do not know the genetic background of your genome\nYou think your genome varies substantially from available reference genomes\nTools: SPAdes, SKESA\n\n\nIn most pathogen-genomics cases, de novo assemblies are the way to go\n\n\n\n\n\n\nTipShould you assemble your genome?\n\n\n\n\n\nIn most cases, yes. But remember that the assembly is a “consensus” of all the reads. So not all the information available in the reads will be represented in the final assembly. For example if you have low-abundance subpopulations in your samples, your assembly will only represent the sequence from the majority population.\n\n\n\n\n\n\n\n\nSimilar to fastp, lets look at the usage instructions for shovill\nshovill --help\nBased on the --help page, we can see that the essential requirements for running shovill are:\n\n--R1 : Paired-end read 1 (R1) file as input\n--R2 : Paired-end read 2 (R2) file as input\n--outdir : Path to the output directory that will be created to store outputs\n\nshovill is actually a software that comprises multiple other software. This type of tool is commonly referred to as a WRAPPER.\nIn this case, shovill is not the name of the actual assembly software, but it lets us choose between multiple assembly software.\nLet us choose skesa as the assembler. The default is another tool called spades but skesa is faster\n\n--assembler: Option to choose assembler, set to skesa\n\nGenome assembly tools may also use a lot of CPU and RAM, if you are running this on your personal computer, you many not have a lot of resources to spare, so it is a good idea to limit this using the --cpus and --ram parameters\nPutting it all together:\nshovill \\\n--R1 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_1_trimmed.fastq \\\n--R2 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_2_trimmed.fastq \\\n--outdir NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_skesa \\\n--assembler skesa \\\n--cpus 4 # Will use at max 4 CPU cores, Set according to your computer \\\n--ram 4 # Will use at max 4GB RAM, set according to your computer\n\n\n\n\n\n\n\nNote\n\n\n\nThis will take time, but you can use the complete assembly already available in NWU_2025_workshop_data/test_datasets/GCA_049744075.1/GCA_049744075.1_ASM4974407v1_genomic.fna for the next steps\n\n\n\nIn the above command, we gave the PATH to our QUALITY FILTERED reads  as input to shovill through the R1 and R2 parameters, specified the name of the output PATH using outdir, and set the assembler to skesa using --assembler skesa\nLook at the different output files using ls and head.\n\n\n\n\nSee how your assembly looks using quast - which is a software that outputs metrics on assembly quality. Take a look at the help page for quast and run it for your assembly!\n\n\nClick to reveal answer\n\nquast \\\n-o NWU_2025_workshop_data/test_datasets/GCA_049744075.1/quast \\\n-t 4 \\\nNWU_2025_workshop_data/test_datasets/GCA_049744075.1/GCA_049744075.1_ASM4974407v1_genomic.fna\n\n-o : Path to output directory\n-t : Number of threads to use\n&lt;contigs&gt; : Path to assembled contigs - This is a Positional argument\n\n\n\n\n\n\n\nTipPositional arguments\n\n\n\n\n\nYou may have noticed that the way you supplied the input files to quast is different from the way it was done for fastp or shovill. For some tools, you will explicitly state a specific parameter that accepts an input file. For example the --R1 parameter accepts the forward read file in shovill. However, with quast there is no such parameter for the input, it automatically considers the argument in the LAST position as the input. These are called POSITIONAL ARGUMENTS. Make sure to always check the --help page to see if the software you are using has positional arguments.\n\n\n\n\nTake a look at the reports generated by QUAST\n\n\n\n\nNumber of contigs - the fewer the better. For short-read assemblies, more than 100 is not great, more than 500 is bad.\nLargest contig size - the larger the better. You want your largest contig to represent as much of the original genome as possible, this means that a large portion of the genome is represented by one uninterrupted large contig.\nN50 value - the larger the better. This refers to the shortest contig such that the sum of the lengths of all contigs of that length or longer constitutes at least 50% of the total assembly length. In other words, imagine sorting all your contigs by length in descending order. How many contigs are required to reach 50% of the original genome’s length? The length of the final contig needed to reach 50% is the N50 value. This value will be higher if you have fewer contigs and longer contigs.\nNs per 100Kbp - the fewer the better. N will be called instead of an actual base (i.e A,T,G,C) if the basecaller is uncertain. You want as few uncertain base calls as possible.\n\n\n\n\n\n\nAnnotation is the process of identifying protein coding genes and other significant regions such as CRISPR sites or ncRNA\nGenome annotation can be divided into two steps:\n\nStructural annotation : Predicting Open Reading Frames (ORFs), coding regions, regulatory motifs, introns/exons etc.\nFunctional annotation : Attaching a potential biological function to protein-coding genes.\n\nHowever, from a workflow perspective, this is a single step process, especially for prokaryotes such as bacteria as the biological “rules” for what makes a gene is largely solved.\nMost annotation tools rely on a database of known gene sequences. The more similar a gene is in your sample to an existing, well characterized gene sequence, the more likely it is that the gene in your sample also shares the same functional properties.\nHowever, many annotation tools also rely on matching motifs or “patterns” of specific sequences rather than entire genes. These are helpful for annotating more diverged sequences that may not have an exact match in the reference databases.\nThere are several databases ranging from very broad to very specific collections of sequences that can aid in annotation.\n\nNCBI BLAST, DIAMOND: Searches input protein or NA sequence against available sequences of known function (maybe)\nVFDB, CARD, AMRFinderPlus : Identification of known virulence factors and antibiotic resistance genes\nSignalP, LipoP, tmHMM : Identification of signal peptides, lipoproteins and transmembrane proteins from conserved motifs\nKEGG, DAVID, PANTHER: Databases of gene/protein families and subfamilies used for tying biological pathways to genes\nNCBI CDART, SWISS-MODEL, PHYRE2, I-TASSER: Predicting functional domains and protein families from amino-acid sequence and secondary structure elements.\n\nCommon bacterial genome annotation software (such as prokka or bakta) combine multiple databases to provide the most complete annotations possible.\n\n\n\nUse prokka to annotate your assembly!\n\n\nClick to reveal answer\n\nprokka \\\n--outdir NWU_2025_workshop_data/test_datasets/GCA_049744075.1/prokka \\\n--cpu 4 \\\n--prefix GCA_049744075.1_ASM4974407v1 \\\nNWU_2025_workshop_data/test_datasets/GCA_049744075.1/GCA_049744075.1_ASM4974407v1_genomic.fna\n\n--output : Path to output directory\n--cpu : Number of CPUs to use\n--prefix : Output file “prefix” - this prefix will be added to the name of all outputs so that you can identify them\n&lt;contigs&gt; : Path to assembled contigs\n\n\nTake a look at the different output files prokka has generated and their corresponding file extensions!\n\n\n\n\n\n\nTipProkka vs Bakta - Use Bakta\n\n\n\n\n\nProkka used to be the industry standard bacterial genome annotation tool but it has not been updated in a long time. Prokka is still a good choice and is widely used, but Bakta is now the more up-to-date tool and it is fast becoming the standard. Prokka is faster and easier to run which is why we are using it for this workshop but for your research work you should use Bakta.\n\n\n\n\n\n\n\n\nSNP calling or variant calling is the process of identifying nucleotide differences amongst your samples\nCalling SNPs is critical for pathogen surveillance applications such as tracking the spread of strains responsible for outbreaks, identifying transmission events between hosts, and conducting genome-wide association studies (GWAS), in which SNPs are associated with phenotypes of interest (e.g., virulence, resistance to antimicrobials)\nThe most common approach to SNP calling relies on a reference genome, which is an assembled genome that is closely related to the genomes of interest.\nThis means and SNPs are identified relative to that reference. Therefore the choice of reference is critical for downstream analyses.\nIdeally, the reference genome used must be closely related to your sample collection, be of high quality (preferably a complete genome), and be associated with a publication ( or have good metadata) such that the reference strain has been characterized well.\n\n\n\n\nMapping reads to a reference genome GalaxyTraining Network, CC-BY-4.0\n\n\nYour input data can either be assemblies (.fasta) or post-QC reads (.fastq). SNP calling with assemblies will be faster but with reads will be more accurate. Different SNP calling tools can accept different inputs and you must choose one according to your needs and what type of data you have available.\nIn general, for bulk comparison of several isolates from a mix of sources where you are less interested in the actual functional effects SNPs and more about the overall similarity across your isolates, or if you need a core genome alignment, you can use assemblies. If you have a small collection of isolates (for eg: from an outbreak or from experimental evolution cultures) where calling accurate SNPs, as well as information regarding exact positions of SNPs and the potential mutations are important, you should use reads.\nWe will discuss different SNP calling pipelines, and other key considerations during SNP calling in a later module. For now since we are only working though a single example genome, let us run a basic SNP calling pipeline using snippy with a reference genome.\n\n\nUse snippy to identify variants in your assembly compared to the reference genome!\nIf you have run setup.sh from Module 0, you should have a folder called reference_genome inside test_datasets. Snippy can use either the genome assembly (_genomic.fna) or the Genbank annotation (genomic.gbff) as the reference.\nYou can also either provide the post-QC reads from fastp or the assembled contigs from shovill as input.\nCheck the snippy help page for how to perform each combination of input file(s) and reference file!\n\n\nClick to reveal answer\n\nsnippy \\\n--cpus 4 \\\n--ram 4 \\\n--outdir NWU_2025_workshop_data/test_datasets/GCA_049744075.1/snippy \\\n--reference NWU_2025_workshop_data/test_datasets/reference_genome/genomic.gbff \\\n--ctgs NWU_2025_workshop_data/test_datasets/GCA_049744075.1/GCA_049744075.1_ASM4974407v1_genomic.fna\n\n# OR\n--reference NWU_2025_workshop_data/test_datasets/reference_genome/GCA_000486855.2_ASM48685v2_genomic.fna \\\n--R1 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_1_trimmed.fastq \\\n--R2 NWU_2025_workshop_data/test_datasets/fastq_download/SRR32528508_2_trimmed.fastq\n\n--ram : The maximum amount of RAM to use\n--cpus : Number of CPUs to use\n--reference : Path to reference file in either .fasta or .gbff format\n--R1 & --R2: Path to post-QC reads\n\nOR\n\n--ctgs : Path to assembled contigs\n\n\nTake a look at the different output files snippy has generated and their corresponding file extensions! If you ran both read-based and assembly-based variant calling, take a look at the list of variants reported in each case. Are there differences? How about when you used a .fasta reference vs a .gbk ? Is the output any different?"
  },
  {
    "objectID": "module2.html#automating-processing-of-multiple-samples",
    "href": "module2.html#automating-processing-of-multiple-samples",
    "title": "Module 2",
    "section": "",
    "text": "In this module, we started with raw fastq reads from the sequencer, assembled the reads into a genome, identified genes in this genome, and compared this genome to a reference to identify SNPs. We performed these steps for one sample sequentially. However, what if you have multiple samples, all of which need to undergo the same steps?\nYes, you can simply copy-paste the above code, changing the input names accordingly for each sample you need to process. While this may work for a handful of samples, what if you have hundreds to process?\nThis is where scripting comes in. You can pre-write a set of instructions, and have those same instructions be repeated for all your samples.\n\n\nTake a look at this example bash script below. This is a script to perform genome annotation on a given assembly. Create a .txt file called prokka_script.sh and save the contents below into it (you can actually name it whatever you want, just make sure to give it the .sh extension so that you know it is a bash script).\n#!/bin/bash \n\nfpath=$1\n\nfname=$(basename ${fpath} _genomic.fna)\n\n# Running prokka\nprokka \\\n--outdir ${fname}_prokka \\\n--cpu 4 \\\n--prefix ${fname} \\\n--fast \\\n${fpath}\nLets go though this script line by line\n\n#!/bin/bash - this is called the shebang line. This line lets your computer know that this is a bash script.\n$fpath=$1\n\n$1 is a positional argument. It refers to the FIRST argument provided after calling the script. Similarly, $2 automatically refers to the second argument provided, $3 the third, and so on. But since prokka accepts needs only one input (the assembly), we only need $1 for this script.\nfpath is a new variable we are creating for the path to the input file. You can call this whatever you want, but just make sure to have variable names that make sense so that you know what it is referring to (calling variables x or y or var etc is bad practice!)\nfpath=$1 means fpath equals whatever is in $1. If we input the path to the fasta file that we need annotated as the first positional argument, fpath will now carry the path.\n\nfname=$(basename $fpath _genomic.fna)\n\nbasename is a bash command that takes the base name of the file that follows it. That means if you supply a filename with the absolute path, basename will keep only the actual name of the file, not the full path.\n${fpath} - we created a variable called fpath previously to hold the path to our input fasta file. Now we are calling the variable. When you want to call a variable in bash, you refer to the variable name after the $ sign enclosed in {}.\nbasename ${fpath} means the basename command will take the name of the input fasta file and not any file paths that may be associated with it.\nbasename ${fpath} _genomic.fna - we have now suppled _genomic.fna as an additonal argument to basename after ${fpath}. This tells basename the file suffix to also be removed. This may or may not be necessary depending on your input file. Since in our case all our files have the suffix _genomic.fna we can remove that too to only keep the actual name. Try it on the command line!\n\n\nbasename NWU_2025_workshop/NWU_2025_workshop_data/test_datasets/salmonella_assemblies/GCA_049744875.1_ASM4974487v1_genomic.fna _genomic.fna\n\nGCA_049744875.1_ASM4974487v1\n\nfname=$(basename ${fpath} _genomic.fna)\n\nWe enclose the basename command inside $() because we want to store the output of the command into a variable, and this variable will be called fname.\nThe reason we are storing the basename of our input file is because we want a way to control the name of the output file. Since we want to use this script for potentially any input fasta, we want to save the output file with an appropriate name that matches the input, so that we know which input file was used to generate which output. This is especially important when you are working with hundreds to thousands of samples as you always want to know exactly how each file was generated.\n\nprokka\n\nWhat follows is the same prokka script as before, but this time instead of directly specifying the name of the input fasta, we have specified the name of the variable.\nSimilarly, for the --outdir parameter, we have specified the basename of the input (${fname}) and appended _prokka to it, so that when we see the output directory we know it was created by prokka.\nWe are also providing ${fname} as the --prefix so that all files created by prokka will have the basename of the input file, again this is to match each output file with the corresponding input.\nFinally we provide ${fpath}, the actual input fasta file (with the absolute path) as the final argument for prokka (Remember that this is a positional argument for prokka so it has to be last)\n\n\nTo run this script on one sample, we can do\nchmod 755 ./prokka_script.sh # This makes the script executable, you only have to do this once\n\n./prokka_script.sh NWU_2025_workshop_data/test_datasets/salmonella_assemblies/GCA_049744875.1_ASM4974487v1_genomic.fna\n\nThis will run prokka for the sample GCA_049744875.1_ASM4974487v1_genomic.fna and save the output to a file called GCA_049744875.1_ASM4974487v1_prokka in the working directory.\n\n\n\n\nWe have still run it for only one sample, but now that we have scripted it, we can automate running this script for a list of samples. The simplest way to do this is using a for loop. This simply means we run the same set of instructions for each item in the list.\nFor loops in general follow this basic structure\nfor item in &lt;list of items&gt;\n  do \n  &lt;set of instructions&gt;\n  done\nThis means, for each item in a list/sequence, you can state a set of instructions to be performed, and it will end automatically when the last item in your list has been processed. The word item in the code above is a placeholder, it can be called anything you want, it is just a way to refer to a single item in the list of items.\nBelow is a simple bash for loop\nfor assembly in $(ls NWU_2025_workshop_data/test_datasets/salmonella_assemblies/); do\n  ./prokka_script.sh NWU_2025_workshop_data/test_datasets/salmonella_assemblies/${assembly}; \n  done\nExplanation:\n\nfor assembly in $(ls NWU_2025_workshop_data/test_datasets/salmonella_assemblies/); do\n\nls NWU_2025_workshop_data/test_datasets/salmonella_assemblies/ : Lists all the files in a folder. There are five assemblies provided in the test datasets. This is enclosed in $() because this is the  we want our for loop to go over.\nfor assembly in : assembly here is just what we are calling the variable. As the for loop goes over each item from the ls command, that item will be stored in a variable called assembly. As soon as one item is done being proccessed, the next item will now take its place in the assembly variable.\n; do : This is simply the bash for loop syntax. We are telling it to perform (or do) the following instructions\n\n./prokka_script.sh NWU_2025_workshop_data/test_datasets/salmonella_assemblies/${assembly} : Just like we ran the script before, we are calling prokka_script.sh and giving it as input whatever value is stored in ${assembly}, which in this case is the name of the assembled genome fasta file. We must provide the full path to the file so that prokka can correctly find it in the right location.\n; done : This again is part of for loop syntax. The instructions that need to be looped should be in-between do and done so that the loop knows when to start and when to stop.\n\n\n\namrfinder is a software used for annotating Anti-Microbial Resistance genes from a given genome.\nIf you performed the setup.sh step in Module0, you will have a set of 5 different assemblies in the folder NWU_2025_workshop_data/test_datasets/salmonella_assemblies/ . Write a loop to perform AMRFinderPlus on these five assemblies.\n\n\nClick to reveal answer\n\nfor i in $(ls NWU_2025_workshop_data/test_datasets/salmonella_assemblies/); do\nbname=$(basename NWU_2025_workshop_data/test_datasets/salmonella_assemblies/${i} _genomic.fna); \namrfinder --threads 1 --nucleotide NWU_2025_workshop_data/test_datasets/salmonella_assemblies/${i} --output ${bname}_amrfinderplus.tsv;\ndone"
  },
  {
    "objectID": "module0.html",
    "href": "module0.html",
    "title": "Module 0",
    "section": "",
    "text": "To follow this course, you will need to install a few things."
  },
  {
    "objectID": "module0.html#shell",
    "href": "module0.html#shell",
    "title": "Module 0",
    "section": "Shell",
    "text": "Shell\n\nWindows\n\nEnable Windows Subsystem for Linux (WSL)\n\nWinkey (⊞) + r to open Windows Run\nType “control panel” and press Enter (↵)\nClick “Programs”\nClick “Turn Windows features on or off”\nCheck boxes for “Virtual machine platform” and “Windows subsystem for Linux”\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis might already be enabled depending on the exact version of windows. If that is your case you can move on to the next step\n\n\n\nInstall a Linux distribution\n\nWinkey (⊞) + r to open Windows Run\nType “powershell” and press Enter (↵)\nIt should display a prompt ending in &gt;\n\nFor example: PS C:\\Users\\username&gt;\n&gt; means the terminal is ready to receive a command\n\nType wsl --update to install the latest update if available\nThen, once &gt; is visible again, type wsl --install -d Debian\nThis will install a distribution of Linux called ‘Debian’ (this will take up roughly 100 Mb of space on your computer)\nOnce the installation completes, it will prompt for a username and password\n\nThe password won’t be visible when you type. Don’t worry, that’s by design.\n\nOnce entered, the Linux terminal will launch with a prompt ending in $\n\nSimilar to &gt; in the Windows terminal, $ means the Linux terminal is ready to receive a command\n\nClose the window\n\n\nThis is how it should look in your terminal:\nPS C:\\Users\\username&gt; wsl --update\nChecking for updates.\nThe most recent version of Windows Subsystem for Linux is already installed.\nPS C:\\Users\\username&gt; wsl --install -d Debian\nInstalling: Debian GNU/Linux\nDebian GNU/Linux has been installed.\nLaunching Debian GNU/Linux...\nInstalling, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username: your_username\nNew password:\nRetype new password:\npasswd: password updated successfully\nInstallation successful!\nusername@PCname:~$\n\n\n\n\n\n\nNote\n\n\n\nHere we installed Debian, but there are many other Linux distributions available. Ubuntu is the most popular, but we are using Debian because it is lightweight.\n\n\n\nLaunch a dedicated Linux terminal\n\nWinkey (⊞) + r to open Windows Run\nType “Debian” and press Enter (↵)\nA new Terminal window will open running Debian\nThis will be the primary working environment\n\n\n\n\nLinux\n\nOpen the built-in terminal\n\nPress control + alt + T\n\n\n\n\nMacOS\n\nOpen the Mac terminal\n\nPress command + space to open the search bar\nType “terminal” and press return"
  },
  {
    "objectID": "module0.html#installing-conda",
    "href": "module0.html#installing-conda",
    "title": "Module 0",
    "section": "Installing Conda",
    "text": "Installing Conda\nConda is a package management software that provides a convenient way to install other packages and their required dependencies, as well as manage virtual environments. For this workshop, we will install Miniconda, which comes with conda and a set of other essential packages.\nThe official Miniconda installation instructions can be found here - https://www.anaconda.com/docs/getting-started/miniconda/install\nFeel free to navigate the above website and find the instructions appropriate for your system if you’re comfortable. If not, follow the instructions below.\n\nOpen the terminal and create a new folder called “miniconda” in your home folder\n\nmkdir ~/miniconda\n\n\n\n\n\n\nTip\n\n\n\n\nmkdir : Command to make a new “directory”. “Directory” and “Folder” are functionally the same and can be used interchangeably.\n\n~/ : is the shortcut for your default home directory.\n\nThis can be replaced with the path to any other location on your computer if you want to create a new folder somewhere else instead.\nYou can type echo $HOME and hit Enter to see the true path to your default home directory.\n\nminiconda : name of the new directory we want to create.\nmkdir ~/miniconda : “Make a new directory called miniconda in the location ~/”\n\n\n\n\nNavigate to the newly created miniconda directory\n\ncd ~/miniconda\n\n\n\n\n\n\nTip\n\n\n\n\ncd : Command to “Change directory”, i.e navigate from the current working directory to a different directory.\n\nYou can type pwd and hit Enter to see the path to the current working directory.\nYou can type ls and hit Enter to list the contents of the current directory.\n\n~/miniconda : Path to the directory you want to navigate to\ncd ~/miniconda : “Change the current working directory to the location ~/miniconda`\n\n\n\n\nSelect the installer:\n\nWindows and Linux: Navigate to https://repo.anaconda.com/miniconda/, right-click Miniconda3-latest-Linux-x86_64.sh from the list and copy the link.\n\n\n\n\n\n\nNote\n\n\n\nWindows users will still be using the Linux version as we will be working on WSL (see section Windows) instead of the Windows terminal.\n\n\nMacOS: Navigate tohttps://repo.anaconda.com/miniconda/, right-click Miniconda3-latest-MacOSX-x86_64.sh if you have an INTEL Mac, or right-click Miniconda3-latest-MacOSX-arm64.sh if you have a M Series Mac, and copy the link.\n\n\n\n\n\n\nNoteHow to find out which CPU you have\n\n\n\nType uname -m in your terminal. If it returns x86_64, you have Intel. Otherwise, you have M series\n\n\n\nDownload the miniconda installer; type curl , and then paste the link you copied. For example, if you copied the “Miniconda3 Linux 64-bit” link, then the command will be\n\ncurl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n\n\n\n\n\nTip\n\n\n\n\ncurl : Command for retrieving data from URLs\n-O : Activates the O flag, which means the contents of the file in the specified URL will be saved with the same name\n\nTo see the list of all possible options and their explanations for curl, type man curl and hit enter. This will open the “Manual” for the curl command.\n\nhttps://.... : Link to the file to download\ncurl -O https://... - “Download the corresponding file from this URL”\n\n\n\nThis will download a file with the extension .sh, which signifies that it is a bash script.\n\nExecute the downloaded bash script. For example, if you downloaded the Miniconda3-latest-Linux-x86_64.sh file, the commands will be:\n\nbash Miniconda3-latest-Linux-x86_64.sh -u -b -p ~/miniconda/\n\n\n\n\n\n\nTip\n\n\n\n\nbash : Command to execute the bash script\nscript_name.sh : Name of the script to execute; in our case, the name of the script is Miniconda3-latest-Linux-x86_64.sh . You can then optionally specify flags unique to the script being executed. In the case of our script,\n\n-u : Activates the “Update” flag; updates an existing directory with new contents\n-b : Activates the “Batch” flag, which will execute the script without requiring any prompts from the user. You can run the above command without -b to see what that looks like.\n-p : Activates the “Prefix” flag; unlike the previous flags this one requires an input. Here, it is the prefix to be added to the new content that will be created. We specified ~/miniconda/ as the prefix, meaning all new files created by this script will be in the location ~/miniconda\n\n\n\n\nThis would install conda and a number of other packages. Typing ls ~/miniconda/ will show you the new contents that have been created in the folder.\n\nNext, run the following to “initialise” conda:\n\n~/miniconda/bin/conda init\nThis makes executing conda commands simpler and faster. Read more about initialization here:\n\nClose and reopen the terminal.\nEnter conda -V ; if the installation went correctly, you should see the conda version number. For example:\n\nconda 25.7.0"
  },
  {
    "objectID": "module0.html#setting-up-the-conda-environment",
    "href": "module0.html#setting-up-the-conda-environment",
    "title": "Module 0",
    "section": "Setting up the conda environment",
    "text": "Setting up the conda environment\nNext we will set up a conda virtual ‘environment’ - think of this as a box inside your computer where all the software you need for a particular project is contained. If you have multiple projects, you can set up multiple independent boxes that will not interfere with each other. This also allows you to simultaneously maintain different versions of the same software if you need to.\nFor the workshop, we have created a conda environment with many standard bioinformatics packages (available here). You can use the following command to load this environment onto your computer.\ncurl -O https://vishnuraghuram94.github.io/NWUPathogenGenomicsWorkshop/NWU_2025_workshop.yml\n\nconda config --set channel_priority flexible\n\nconda env create -f NWU_2025_workshop.yml\nThe environment can be activated with the following command:\nconda activate NWU_2025_workshop\nOnce you have activated the environment, you will see the name of the environment in your bash prompt. It might look something like this\n# Before activating\n(base) username:~$\n\n#After activating\n(NWU_2025_workshop) username:~$"
  },
  {
    "objectID": "module0.html#downloading-example-datasets",
    "href": "module0.html#downloading-example-datasets",
    "title": "Module 0",
    "section": "Downloading example datasets",
    "text": "Downloading example datasets\nWhile the environment is active, download the setup script from the github page and run it. This will setup some folders and download some sample datasets which we will be using for the rest of this workshop. It will take some time, start the script and then feel free to grab a coffee! Once you see the prompt again, it means the script is done. The script will also output a “Setup successful” message.\nThis setup will take 4GB of space so make sure you have sufficient space available\ncurl -O https://vishnuraghuram94.github.io/NWUPathogenGenomicsWorkshop/setup.sh\n\nbash setup.sh\n\n\n\n\n\n\nNote\n\n\n\nYou must activate an environment before you are able to use the tools within that environment. By default, you will be on the base environment. While it is fine to install packages while in the base environment, it can soon get crowded with multiple conflicting packages and lead to issues later down the line. Therefore it is good practice to keep the base environment as clean as possible while creating separate environments for different tools/projects. You can deactivate the current environment and go back to base using the command:\nconda deactivate"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Bacterial Pathogen Genomics Workshop",
    "section": "",
    "text": "The goal of this workshop is to provide participants with a solid foundation in pathogen genomics, covering key bioinformatics tools and concepts, ensuring a mix of theoretical learning and hands-on exercises.\n\n\n\nGoal: Get everyone on the same page regarding command-line basics, the bacterial genomics workflow, and basic tools used in pathogen genomics.\n\n\n\nWhat is the command line? Why is it essential in genomics?\nFile structure, directories, and navigating the command line.\nKey commands (e.g., ls, cd, cp,mkdir, etc.)\nSimple text processing (using grep, sed, tr)\nHands-on exercises combining different commands, piping and output redirection.\n\n\n\n\n\nOverview of the genomic analysis pipeline for bacteria: QC, genome assembly, annotation, variant calling\nWhy do we perform each step? What are the key questions each step answers in pathogen genomics?\nParticipants will run fastp, Shovill, bakta and snippy (or snippy-multi) on sample data.\nWriting a simple loop to automate processing multiple samples\n\n\n\n\n\n\nGoal: Introduce public pathogen surveillance databases and tools for genome comparison.\n\n\n\nExploring of NCBI, SRA, BioSamples, BioProjects.\nThe importance of using public databases in surveillance and research\nRetrieving genomic data from public databases (ncbi-genome-download,ncbi-datasets,sratools).\nExploring Enterobase.\n\n\n\n\n\nMLST and how it’s used in pathogen typing\nIntroduction to genome distance estimation methods (MASH, ANI, SNP distances)\nParticipants will use MLST to assign sequence types to bacterial genomes.\nParticipants will run mash,skani, and snp-dists calculate genome distances from sample data.\n\n\n\n\n\nSetting up RStudio, installing essential packages, reading data tables\nData visualization in RStudio: ggplot2, ggtree, and other essential packages.\nParticipants will create basic visualizations using ggplot2 (e.g., bar plots, scatter plots).\n\n\n\n\n\n\nGoal: Introduce pangenomes and phylogenies\n\n\n\nWhat is a phylogenetic tree? How is it used to study pathogen evolution?\nDiscussion on tree-building methods (e.g. Neighbor-Joining, Maximum Likelihood)\nParticipants will generate a Neighbour-Joining tree using mashtree and a ML tree using IQ-TREE.\nWhat is pangenomics? Why is it important for studying pathogen diversity?\nOverview of pangenomic analysis workflows and tools (e.g., Panaroo for bacterial pangenomes).\n\n\n\n\n\n\nGoal: Dive deeper into RStudio for advanced data visualization techniques and analysis.\n\n\n\nAdvanced visualizations: Heatmaps, PCA/UMAP, faceting.\nAdvanced ggtree usage with gheatmap\nExploring visualization of SNP distances, pangenomes, and phylogenetic trees.\n\n\n\n\n\nBrief overview of writing pipelines, NextFlow, SLURM\nDiscussing scaling up and automation (using Bactopia)\n\n\n\n\n\n\nGoal: Allow time for troubleshooting, revisiting any unclear topics, and addressing specific questions.\n\nExtra time in case of any overflow from previous days\nBrief recap of everything covered over the week.\nParticipants can ask specific questions or seek help with problems encountered during the week.\nHelp with more advanced problems that might relate to participants’ own research."
  },
  {
    "objectID": "about.html#day-pathogen-genomics-workshop-schedule",
    "href": "about.html#day-pathogen-genomics-workshop-schedule",
    "title": "Bacterial Pathogen Genomics Workshop",
    "section": "",
    "text": "The goal of this workshop is to provide participants with a solid foundation in pathogen genomics, covering key bioinformatics tools and concepts, ensuring a mix of theoretical learning and hands-on exercises.\n\n\n\nGoal: Get everyone on the same page regarding command-line basics, the bacterial genomics workflow, and basic tools used in pathogen genomics.\n\n\n\nWhat is the command line? Why is it essential in genomics?\nFile structure, directories, and navigating the command line.\nKey commands (e.g., ls, cd, cp,mkdir, etc.)\nSimple text processing (using grep, sed, tr)\nHands-on exercises combining different commands, piping and output redirection.\n\n\n\n\n\nOverview of the genomic analysis pipeline for bacteria: QC, genome assembly, annotation, variant calling\nWhy do we perform each step? What are the key questions each step answers in pathogen genomics?\nParticipants will run fastp, Shovill, bakta and snippy (or snippy-multi) on sample data.\nWriting a simple loop to automate processing multiple samples\n\n\n\n\n\n\nGoal: Introduce public pathogen surveillance databases and tools for genome comparison.\n\n\n\nExploring of NCBI, SRA, BioSamples, BioProjects.\nThe importance of using public databases in surveillance and research\nRetrieving genomic data from public databases (ncbi-genome-download,ncbi-datasets,sratools).\nExploring Enterobase.\n\n\n\n\n\nMLST and how it’s used in pathogen typing\nIntroduction to genome distance estimation methods (MASH, ANI, SNP distances)\nParticipants will use MLST to assign sequence types to bacterial genomes.\nParticipants will run mash,skani, and snp-dists calculate genome distances from sample data.\n\n\n\n\n\nSetting up RStudio, installing essential packages, reading data tables\nData visualization in RStudio: ggplot2, ggtree, and other essential packages.\nParticipants will create basic visualizations using ggplot2 (e.g., bar plots, scatter plots).\n\n\n\n\n\n\nGoal: Introduce pangenomes and phylogenies\n\n\n\nWhat is a phylogenetic tree? How is it used to study pathogen evolution?\nDiscussion on tree-building methods (e.g. Neighbor-Joining, Maximum Likelihood)\nParticipants will generate a Neighbour-Joining tree using mashtree and a ML tree using IQ-TREE.\nWhat is pangenomics? Why is it important for studying pathogen diversity?\nOverview of pangenomic analysis workflows and tools (e.g., Panaroo for bacterial pangenomes).\n\n\n\n\n\n\nGoal: Dive deeper into RStudio for advanced data visualization techniques and analysis.\n\n\n\nAdvanced visualizations: Heatmaps, PCA/UMAP, faceting.\nAdvanced ggtree usage with gheatmap\nExploring visualization of SNP distances, pangenomes, and phylogenetic trees.\n\n\n\n\n\nBrief overview of writing pipelines, NextFlow, SLURM\nDiscussing scaling up and automation (using Bactopia)\n\n\n\n\n\n\nGoal: Allow time for troubleshooting, revisiting any unclear topics, and addressing specific questions.\n\nExtra time in case of any overflow from previous days\nBrief recap of everything covered over the week.\nParticipants can ask specific questions or seek help with problems encountered during the week.\nHelp with more advanced problems that might relate to participants’ own research."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pathogen genomics workshop",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nThis page is under construction"
  },
  {
    "objectID": "module1.html",
    "href": "module1.html",
    "title": "Module 1",
    "section": "",
    "text": "This module will cover the basics of navigating the bash terminal, go through some key bash commands, and common file formats."
  },
  {
    "objectID": "module1.html#what-is-the-command-line-why-is-it-essential",
    "href": "module1.html#what-is-the-command-line-why-is-it-essential",
    "title": "Module 1",
    "section": "What is the command line? Why is it essential?",
    "text": "What is the command line? Why is it essential?\n\nThe command line is a way to interact with your computer simply by typing on your keyboard, without clicking on icons\nInstead of manually clicking through multiple windows and files, you type out “commands” and your computer does them for you\nThis allows you to easily reproduce what you did before, automate several tasks, and handle large datasets too cumbersome for a human to manually go through.\nThis also provides a great foundation for learning other programming languages\nSeveral bioinformatic tools do not have a graphical user interface (GUI) and need to be used from the command-line"
  },
  {
    "objectID": "module1.html#exploring-the-file-system",
    "href": "module1.html#exploring-the-file-system",
    "title": "Module 1",
    "section": "Exploring the file system",
    "text": "Exploring the file system\nTo ‘move’ around your computer, typically you would use the built-in file explorer, where you would point and click to navigate through different files and folders.\nWe can also perform the same actions from the terminal by simply typing out the PATH to each file or folder\nThe PATH to a file is the exact location of that file on the computer.\nIt is very important to know at all times exactly where you are in the file system as well as where the files you are working with are.\n\nThe working directory\nOpen the terminal and create an empty file\ntouch example.txt\nWe created a file called example.txt using the touch command. Here, we simply specified the name of the file to be created, but did NOT specify where the file should be created. This means, the file got created in the WORKING DIRECTORY.\nIf you are creating or referring to a file without specifying the PATH , your computer will assume you are pointing to the current WORKING DIRECTORY.\nThe WORKING DIRECTORY refers to the directory you are currently in.\nThe WORKING DIRECTORY can also be signified as ./ .\nWhile ./ will always point to the current WORKING DIRECTORYy, it is merely a shorthand that is referring to the actual PATH to your WORKING DIRECTORY directory.\nThe pwd command or ‘Print Working Directory’ prints the path to the current working directory. It might look something like this\npwd\n/mnt/c/Users/username/project\n\nThe path to any location always starts with / which signifies the root directory. The root is the starting location and it is as far back in the filesystem as you can go.\nIf you imagine the filesystem as a tree with each branch being a different folder, the root directory is the base of the tree trunk\nFrom the root directory /, we are inside the directory mnt, followed by c, followed by Users and so on.\nWhen we created example.txt, we created it from the project directory.\nThis means, the PATH to example.txt is\n\n/mnt/c/Users/username/project/example.txt\n\n\n\n\n\n\nTipDirectory vs. Folder\n\n\n\n\n\nBoth are generally used interchangably though directory is the technically correct term.\n\n\n\n\n\nThe home directory\nWhen you open your terminal, your default location will be your HOME directory\nThis can be signified as ~/ .\nWhile ~/ will always point to your default HOME directory, it is merely a shorthand that is referring to the actual PATH to your HOME directory.\nThis path can be seen using:\necho $HOME\n/mnt/c/Users/username\n\nThe echo command simply prints out the following text\n$HOME refers to the built-in variable called HOME\n$HOME contains a value, which is the path to your HOME directory\nIn this case, my home directory is /mnt/c/Users/username\nSo if I want to point to example.txt, another way for me to do it is:\n\n~/project/example.txt\n\n\nChanging directories\nThe cd command (or “change directory”) allows you to move between different directories\nFor example, if we would like to move to our home directory, we can type\ncd ~/\n\n\n\n\n\n\nNote\n\n\n\nIn the above example, you would directly move to your home directory no matter where in the filesystem you were, as ~/ always refers to your home directory.\n\n\nNow our current working directory is our home directory.\nIf we would like to switch to the project directory, we can type\ncd project\nWe can move out one level from the current working directory using ../ . Similar to how ./ always refers to the current directory, ../ always refers to one directory above the current directory.\nFor example, if our current directory is /mnt/c/Users/username/project/\ncd ../\npwd\n/mnt/c/Users/username/\ncd ../ takes us one directory above project, which in this case is username. If we were to repeat this, we would go up one more directory, taking us to Users:\ncd ../\npwd\n/mnt/c/Users/\nTherefore ./ and ../ are shorthands that refer to paths that change depending on the current working directory. In other words, they are RELATIVE .\n\nAbsolute vs. Relative Paths\nThere are two main types of paths when referring to files and directories: ABSOLUTE and RELATIVE paths.\nAbsolute Paths always start from the root directory / and specify the exact location of a file or folder in the filesystem. For example, the absolute path to example.txt is:\n/mnt/c/Users/username/project/example.txt\nRelative Paths are paths that are relative to the current working directory. For instance, if you are currently in the username directory, you can refer to example.txt by:\n./project/example.txt\nIf you are in project , and you want to refer to another file another_example.txt that is in Users, you can refer to it relative to your current location such as\n../../another_example.txt\n../../ goes two levels above project to reach Users and then refers to another_example.txt .\n\n\n\n\n\n\nTipWhy You Should Practice Using Absolute Paths\n\n\n\n\n\nIt’s generally a good idea to use absolute paths when you’re working with files or directories, especially when writing scripts or commands that could be run from different locations. This ensures that you are always referring to the exact file, no matter where you are in the filesystem.\n\n\n\n\n\nThe autocomplete feature\nWhen you are typing out filenames or directory names in the shell, it will attempt to automatically complete what you are typing with the file/directory names WITHIN THE DIRECTORY YOU ARE CURRENTLY IN. For example, say you want to change to the directory /mnt/c/Users/username/ .\nWhen you begin typing /mnt/c/Users/us and then press the tab (⭾) button on your keyboard, it will automatically complete to /mnt/c/Users/username/ .\nIf you have multiple folders that begin with user, it will complete to the furthest possible character. For example if you have the following files in your Users directory\n$: ls Users\n\nuser1 user2 username\nThe autocomplete feature will complete /mnt/c/Users/us to /mnt/c/Users/user, after which you can type n to make it /mnt/c/Users/usern and then press tab again, which will then complete it to /mnt/c/Users/username\nThe autocomplete feature works the same way for files, for example /mnt/c/Users/username/project/exa will complete to /mnt/c/Users/username/project/example.txt .\n\n\n\n\n\n\nWarningThe shell is CASE-SENSITIVE\n\n\n\n\n\nexample.txt and Example.txt will be treated as different files. If you type exa and press tab, it will never complete to Example.txt.\n\n\n\n\n\n\nOther useful commands\n\nmkdir : Makes a new directory called dir1 in the current directory (project)\nmkdir dir1\nI recommend making a separate project directory (or any name of your choosing) for this workshop to perform all upcoming exercises.\n\n\n\n\n\n\nWarningDO NOT USE SPACES IN FILE/FOLDER NAMES\n\n\n\n\n\nIn general it is good practice to not use spaces or special characters ( such as ?!&lt;&gt;,[]{}() ) in your file or folder names as usually, spaces and special characters have a SPECIAL MEANING in the command line and stand for specific operations. This can lead to the system getting confused when you refer to a folder with a special character, as it will assume you are performing an operation that the character stands for. For example, the space character is usually the separator for different options/modifiers for command line tools. Use _ instead of spaces.\n\n\n\n\n\nls : Lists all contents in the specified directory.\nIn the below example we list all the contents in the directory project\nls /mnt/c/Users/username/project/\ndir1\nexample.txt\n\n\ncp : Creates a copy of a file.\nThe below example copies example.txt to dir1 .\ncp example.txt ./dir1/  \n\n\nmv : Moves a file from one location to another.\nThe below example moves example.txt from dir1 to dir2.\nmkdir dir2\nmv ./dir1/example.txt ./dir2/  \n\n\ncat : Display all the contents within a file.\nTo display all the contents that is within example.txt:\nls cat example.txt\nIn our case since we did not write anything into example.txt it will be empty. Feel free to open example.txt in your text editor, write some text into it and then try out the cat command.\n\n\nhead and tail : Display the first n or last n lines in a file\nBy default head or tail will display the first 10 or last 10 lines of a file.\nhead example.txt \n\n\nrm : Removes (delete) a specified file. In the below example we delete example.txt in dir2\nrm /mnt/c/Users/username/project/dir2/example.txt\n\n\n\n\n\n\nWarningREMOVING IS PERMANENT\n\n\n\nBe very careful when using the rm command as there will be no confirmation dialogue box asking you if you are sure you want to delete the file as you might be used to. The file will simply PERMANENTLY be deleted!\n\n\n\n\nman: Shows the manual (help) page for any command.\nYou can also follow any command with either -h or --help to display the help page. Look up the help page of the above commands to see all the different ways they can be used!\nman ls\nls --help\nFeel free to experiment with the above commands, navigating between different folders, creating and moving files, using absolute and relative paths and so on.\nYou can also add modifiers to each command to slightly change its behaviour. For example ls simply lists the names of all the files in the specified directory but ls -l lists them with a lot more additional information! You can also use multiple modifiers at a time like ls -l -a (What does -a do?) . Make sure to look into the --help pages for the commands to know more.\n\n\n\nEXERCISE: Try to find the modifiers for each of the commands above to do the following :\n\nList files along with the file sizes\n\n\n\nClick to reveal answer\n\nls -sh\n\n\nList files sorted by the time they were created\n\n\n\nClick to reveal answer\n\nls -lt\n\n\nPrint the first 30 lines from a file\n\n\n\nClick to reveal answer\n\nhead -n 30 filename.txt\n\n\nPrint all lines starting from the 3rd line\n\n\n\nClick to reveal answer\n\ntail -n +3 filename.txt\n\n\nCopy or move a directory along with all the contents\n\n\n\nClick to reveal answer\n\ncp -r /path/to/dir /path/to/new_dir\n\n\nDelete a directory\n\n\n\nClick to reveal answer\n\nrm -r /path/to/dir"
  },
  {
    "objectID": "module1.html#files-and-file-extensions",
    "href": "module1.html#files-and-file-extensions",
    "title": "Module 1",
    "section": "Files and File Extensions",
    "text": "Files and File Extensions\nEvery file on your computer has a file extension, which is part of its name and comes after the last period (dot) in the filename. For example, in example.txt, .txt is the file extension.\nFile extensions are used by the operating system to determine what kind of file it is, what program should open it, and how it should be processed. Some common file extensions include:\n\n.txt - Text file\n.csv - A table/dataframe with comma separated values\n.docx - Microsoft Word document\n.pdf - PDF document\n.jpg - JPEG image file\n.sh - Shell script\n\n\nWhy Are File Extensions Important?\nFile extensions are important because they help the system identify how to handle a file. For example: - A .txt file is usually opened with a simple text editor (e.g., Notepad, VSCode, etc.). A .docx file is typically opened in Microsoft Word or similar word processing software.\n\n\n\n\n\n\nNote\n\n\n\nThe extension does not determine the content of a file, just how it is generally interpreted by the system. You can change a file extension, but it doesn’t change the underlying content, though it might prevent the file from being opened correctly. For example you can change the extension of a .jpg image to .txt , but that doesn’t mean you can open it in Notepad as it will still be an image.\n\n\n\n\nPlain Text Files\n\nWhat is Plain Text?\nPlain text files contain ONLY text. There is no additional formatting (such as bold/italics/colours), images, or embedded metadata. These files can be created and edited with simple text editors such as NotePad (Windows), TextEdit (MacOS), gedit (Ubuntu)\n\n\n\n\n\n\nNote\n\n\n\nA word/google docs document with only text still is not a plain text file. They still have a lot of embedded formatting and metadata.\n\n\nPlain text is UNIVERSAL. It is not a proprietary format that can only be read by specific programmes (for example, .docx). Any text editor across any system can read and manipulate plain text files.\nIn bioinformatics, plain text files are preferred for storing data because they can be easily read, processed, and manipulated by a variety of different software. Scripts for some programming languages such as Python, Bash, and R are also in plain text format.\nHowever, not all bioinformatics data comes in plain text. Many data formats are designed for efficient storage, performance, or specific analysis tasks, and they may be binary formats (e.g., BAM). In bioinformatics workflows, plain text formats are still commonly used for various data exchange and reporting.\n\n\n\n\n\n\nNote\n\n\n\nFor some programming languages, the scripts need to be compiled before they can be executed (for eg: C, Rust). While the script itself will be in plain-text, it cannot be executed unless it is compiled into a binary. In the case of Python, R and Bash, compilation is not needed.\n\n\nHere are some of the most common plain text files used in bioinformatics:\n\n\n1. .csv or .tsv\nThis will probably be the most common file format you will be directly interacting with on a regular basis. csv (Comma Separated Values) or tsv (Tab separated values) are used to store tabular data with commas (in csv) or tabs (in tsv) as the separator in-between values in the table. In other words, it is a way to represent a table in plain-text, with rows separated by new lines and columns separated by commas or tabs. This separator is referred to as the delimiter, with tsv files also being referred to as tab delimited files and csv as comma delimited. Many of the common bioinformatic file formats we will be discussing below are also tab delimited files.\nExample .tsv file:\nsample_name   Gene_symbol   Sequence_name\nSAMN09634554    sseK2   type III secretion system effector arginine glycosyltransferase SseK2\nSAMN42760649    mdsB    multidrug efflux RND transporter permease subunit MdsB\nSAMN07714001    golT    gold/copper-translocating P-type ATPase GolT\nSame example as a .csv\nsample_name,Gene_symbol,Sequence_name\nSAMN09634554,sseK2,type III secretion system effector arginine glycosyltransferase SseK2\nSAMN42760649,mdsB,multidrug efflux RND transporter permease subunit MdsB\nSAMN07714001,golT,gold/copper-translocating P-type ATPase GolT\n\n\n\n\n\n\nNote\n\n\n\nA .csv or .tsv may not be the most convenient way for us to visualize a table compared to software like excel that neatly separate each value into individual cells, however when you are working with large datasets having thousands to millions of rows, these format enable extremely fast and efficient processing of the data. Moreover, several bioinformatics tools will use .csv and/or .tsv files as their inputs or outputs. So it is very important you are familiar with handling these files.\n\n\n\n\n2. FASTA\n\nFASTA is one of the most widely used formats for representing biological sequences, such as DNA, RNA, or protein sequences. A FASTA file can consist of multiple sequences. Each entry in a FASTA file has two components :\n\nDescription line: Starts with &gt;, followed by an identifier and optional description.\nSequence: The actual sequence of nucleotides (DNA) or amino acids (protein). The sequence can be in a single line or broken up into multiple lines.\n\nIn a genome FASTA file, each sequence is called a contig . In most cases a genome will not be assembled “perfectly” into a single long stretch of DNA, and will be broken up into contigs. This is covered in detail in Module 2.\nExtensions: .fasta,.fna,.fa,.faa\n\nExample FASTA format:\n&gt;HNLNHC_00005 Oxaloacetate decarboxylase\nGTGCGCGAGGACCTTGGCTTTATCCCGCTGGTGACCCCCACTTCACAGATT\nGTCGGCACCCAGGCGGTGCTCAACGTCCTGACCGGCGAACGCTACAAAACC\n\n&gt;HNLNHC_00010 Oxaloacetate decarboxylase beta chain 2\nATGGAAAGTCTGAACGCCCTGCTTCAGGGCATGGGGCTGATGCACCTTGGC\nGCAGGCCAGGCCATCATGCTGCTGGTGAGCCTGCTGCTGCTGTGGCTGGCG\n\n\n3. FASTQ\n\nThe FASTQ format is an extension of the FASTA format, used for storing raw sequence data with quality scores. Each entry contains four lines:\n\nHeader: Starts with @ followed by the sequence identifier. For paired-end sequencing, this identifier is what keeps track of the “mates”.\nSequence: The actual sequence of nucleotides.\nPlus line: Starts with a + sign (may also followed by the same sequence identifier)\nQuality scores: Per-base quality score corresponding to the sequence in line 2. Each symbol here corresponds to a numeric score for the base call in the corresponding position. Learn more about quality scores here.\n\nExtensions: .fastq,.fq . Typically paired-end fastq reads for a single sample will have the suffix _R1.fastq and _R2.fastq.\n\nExample FASTQ format:\n@SRR16006951.1 1 length=149\nCTGTTCGATATTGCCGCCTTGCGCCCCGCGCCGCTCACCCCGCTGGTGGCATTAATTACCGGCCACTGCGTCAGATCCAAAAGACCGCCGTCAATCAGCGGTTTTAGCGACAACTGCGCTGCGGTTGGATAGCAACCAGGAACCGCAAT\n+SRR16006951.1 1 length=149\nAAAAAEEEEEEAEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEAEEEEEEEEEEAAEAEEEEEEEEEEEEEEEEEEAAEEEEEEEEEEEEEEEEEEEEEEEA&lt;/EEEEE/&lt;EAAAE/&lt;\n@SRR16006951.2 2 length=148\nGTCGCCGTGTTTCTCTCCTTTGATGGCGAACTCGACACCCAGCCGCTGATAGAACAGCTATGGCAAGCGGGCAAGCGCGTCTACCTTCCGGTTCTTCATCCCTTCAGCCCTGGCAACCTCCTGTTCCTGCACTATCATACGCAGAGTG\n+SRR16006951.2 2 length=148\nAAAAAEEEEAEE/EEE/EAEEAEEAEEEEEEA&lt;AEEEEEE//EEAE&lt;EEEA/EE&lt;EAEEEA&lt;AEEE/E&lt;EEEEEAEAE&lt;EA/E&lt;A6E/E/E/E/&lt;E/E&lt;/&lt;A/E&lt;&lt;/EEE/&lt;EE/EAE//A/&lt;&lt;E/E/6&lt;EA/A&lt;/A//&lt;/AE/E/E/\n\n\n5. GFF\n\nGFF or General Feature Format is a tab delimited file with information about genes and other features from a FASTA file.\nOnce a FASTA file is annotated i.e by running some kind of gene finding/functional annotation software, specific regions in the fasta file will be identified as genes (or other genetic features such as non-coding RNA, tRNA and so on)\nA GFF file may vary depending on exactly how it was generated but in general it has the following columns:\n\nMetadata lines starting with ##\nSequence ID/contig ID that was annotated\nThe specific feature (CDS/ncRNA etc)\nThe start and end positions of the feature on the contig\nMore details regarding the annotation\n\nExtensions: .gff, .gff3\n\nExample GFF format :\n##gff-version 3\n##feature-ontology https://github.com/The-Sequence-Ontology/SO-Ontologies/blob/v3.1/so.obo\ncontig_1    CDS     90      1232    ID=HCEKFJ_00005;Name=Biotin/lipoyl-binding protein;product=Biotin/lipoyl-binding protein;\ncontig_46   ncRNA   1728    1828    ID=HCEKFJ_23455;Name=RNAI;gene=RNAI;product=RNAI;\n\n\n4. GenBank\n\nGenBank is also a gene feature/annotation format containing information about genes and other features, but is larger and contains more information than the gff file. For eg:\n\nInformation regarding the person/institution that generated the file with contact information\nIf the record is connected to a specific publication\nAmino acid sequences for each annotated gene.\n\nExtensions: .gbk, .gbff\n\nExample GenBank format : NCBI Sample Genbank Record\n\n\n6. SAM/BAM\n\nSAM (Sequence Alignment/Map) is a tab delimited with information about the alignment of sequences to a reference genome.\nDepending on how the SAM file was generated, the formatting can vary slightly but the first few columns are mandatory. In general all SAM files have\n\nThe identifier of read that was mapped\nThe SAM flag describing certain properties of the mapped read\nThe reference contig ID, mapping quality\nThe mapping position\nA compressed representation of the alignment called CIGAR (for eg: if any insertions/deletions are present and at what position)\n\nSee here for a more detailed explanation of SAM files.\nBAM is the binary equivalent of SAM. BAM files are not in plain-text, they are compressed (and usually also indexed) versions of SAM files. This would make BAM files more efficient in terms of storage and allows fast querying for alignments across specific positions without having to search through the whole file.\n\nExtensions: .sam, .bam . A BAM index will have the extention .bai\n\nExample SAM format:\n@HD VN:1.6 SO:coordinate\n@SQ SN:ref LN:45\nr001 99 ref 7 30 8M2I4M1D3M = 37 39 TTAGATAAAGGATACTG *\nr002 0 ref 9 30 3S6M1P1I4M * 0 0 AAAAGATAAGGATA *\nr003 0 ref 9 30 5S6M * 0 0 GCCTAAGCTAA * SA:Z:ref,29,-,6H5M,17,0;\n\n\n7. VCF\n\nVCF or Variant Call Format is a tab delimited file derived from the SAM file and comprises specifically genetic variation data after alignment.\nA VCF file contains file metadata (lines starting with ##) followed by genetic variants (SNPs/indels) at each position when compared to the reference.\nIn the example below, you can see that the lines starting with ## have information about the file and the INFO and FORMAT columns\nSimilar to the SAM file, the VCF file can have slight differences depending on how it was generated but the first few columns are mandatory.\nFor eg: A vcf file generated from an alignment against an annotated reference will also provide the functional nature of the variant, for eg: if a SNP was found in a coding region, the VCF file will have additional information (eg: Synonymous/Non-synonymous SNP,if indel causes frameshift)\nThe binary equivalent of a VCF file is BCF\nExtensions: .vcf , .bcf\n\nExample VCF Format:\n##fileformat=VCFv4.2\n##reference=GRCh37\n##INFO=&lt;ID=AB,Number=A,Type=Float,Description=\"Allele balance at heterozygous sites: a number between 0 and 1 representing the ratio of reads showing the reference allele to all reads, considering only reads from individuals called as heterozygous\"&gt;\n##INFO=&lt;ID=AO,Number=A,Type=Integer,Description=\"Count of full observations of this alternate haplotype.\"&gt;\n##INFO=&lt;ID=DP,Number=1,Type=Integer,Description=\"Total read depth at the locus\"&gt;\n##INFO=&lt;ID=QA,Number=A,Type=Integer,Description=\"Alternate allele quality sum in phred\"&gt;\n##INFO=&lt;ID=QR,Number=1,Type=Integer,Description=\"Reference allele quality sum in phred\"&gt;\n##INFO=&lt;ID=RO,Number=1,Type=Integer,Description=\"Count of full observations of the reference haplotype.\"&gt;\n##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\"Genotype\"&gt;\n##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=\"Read Depth\"&gt;\n##FORMAT=&lt;ID=RO,Number=1,Type=Integer,Description=\"Reference allele observation count\"&gt;\n##FORMAT=&lt;ID=QR,Number=1,Type=Integer,Description=\"Sum of quality of the reference observations\"&gt;\n##FORMAT=&lt;ID=AO,Number=A,Type=Integer,Description=\"Alternate allele observation count\"&gt;\n##FORMAT=&lt;ID=QA,Number=A,Type=Integer,Description=\"Sum of quality of the alternate observations\"&gt;\n##FORMAT=&lt;ID=GL,Number=G,Type=Float,Description=\"Genotype Likelihood, log10-scaled likelihoods of the data given the called genotype for each possible genotype generated from the reference and alternate alleles given the sample ploidy\"&gt;\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT\nNZ_CP019184.1   39681   .   T   A   682.658 .   AB=0;AO=20;DP=20;QA=780;QR=0;RO=0;TYPE=snp  GT:DP:RO:QR:AO:QA:GL    \nNZ_CP019184.1   40836   .   A   G   682.658 .   AB=0;AO=20;DP=20;QA=780;QR=0;RO=0;TYPE=snp  GT:DP:RO:QR:AO:QA:GL    \nNZ_CP019184.1   48112   .   A   G   682.658 .   AB=0;AO=20;DP=20;QA=780;QR=0;RO=0;TYPE=snp  GT:DP:RO:QR:AO:QA:GL"
  },
  {
    "objectID": "module1.html#working-with-files-in-the-terminal",
    "href": "module1.html#working-with-files-in-the-terminal",
    "title": "Module 1",
    "section": "Working with files in the terminal",
    "text": "Working with files in the terminal\nNow that we went through some common file formats we will be working with, let’s now learn how to manipulate and extract these files from the terminal.\n\nStandard output redirection\nWhen you enter any command in the terminal and see the output on your screen - this output is called the standard output or stdout . For eg: the ls command lists the files and folders in the current directory. Everything that is listed is the stdout\nls \n\ndir1\ndir2\nexample.txt\nThe stdout of any command can be redirected into a file using &gt;.\nls &gt; list_of_files.txt\nNote that there is no stdout printed this time. This is because we redirected the stdout into list_of_files.txt. Now you can see the contents of list_of_files.txt using the cat command that we learned before.\ncat list_of_files.txt\n\ndir1\ndir2\nexample.txt\n\n\n\n\n\n\nWarningTHE &gt; REDIRECTION IS DANGEROUS\n\n\n\n\n\nBe very careful when writing a to file with &gt; as if you happen to specify an existing file, it will OVERWRITE all the contents of the existing file with whatever you are redirecting into it. This process is IRREVERSIBLE and you will lose all the original contents of the file. If you want to append (or add to) an existing file, you can use &gt;&gt; and it will add new lines instead of overwriting the existing lines.\n\n\n\nIt is also possible to redirect the stdout of one command into the input of another command using the “pipe” characer - |. This process is called piping. For example, lets redirect the output of ls to a new command wc . wc prints the number of lines, words, and characters present in the given input. In this case, the input would be whatever the ls command outputs. Here, wc is telling us that there are 3 lines, 3 words and 22 characters.\nls | wc\n\n3  3  22\nPerforming the above command is basically the same as performing:\nls &gt; list_of_files.txt\nwc list_of_files.txt\nBut by using piping, we were able to do it much faster and without writing a new file.\n\n\nExtracting specific information\nThe grep command will search for a string (a specific piece of alphanumeric text) within a plain-text file. It is similar to the ctrl + F or find feature that you many have commonly used to search for text within documents. For example, if we want to search for the string dir in list_of_files.txt, we can do\ngrep \"dir\" list_of_files.txt\n\ndir1\ndir2\ngrep by default will output ALL lines containing the matching string. The string matching is CASE-SENSITIVE, therefore grepping for the string Dir in this case will give an empty result. However if you want grep to perform a CASE-INSENSITIVE search, you can do:\ngrep -i \"Dir\" list_of_files.txt\n\ndir1\ndir2\nIn the above example, grep ignores the case of the string to be searched. There are many such useful modifiers, check man grep or grep --help to see the different behaviours. Another useful modifier is -v for INVERTED matching. This will output only lines that DO NOT HAVE the search string.\ngrep -v \"dir\" list_of_files.txt\n\nexample.txt\nIf grep is similar to the find operation, then sed is the equivalent of find + replace. sed can substitute a specific character, string or pattern with another. By default, sed will replace the first occurrence of the search string in each line. The syntax for sed substitutions is as follows:\nsed 's/search_string/replace_string/' filename\nLets say we want to replace dir in list_of_files.txt with Directory\nsed 's/dir/Directory/` list_of_files.txt\n\nDirectory1\nDirectory2\nexample.txt\nNote that sed is simply performing the specified action in the stdout. It is not actually changing the text within list_of_files.txt. If you would like to REWRITE list_of_files.txt with the specified action, you can perform an in-place substitution with the -i modifier in sed. THIS IS IRREVERSIBLE.\nsed -i 's/dir/Directory/` list_of_files.txt\n\ncat list_of_files.txt\n\nDirectory1\nDirectory2\nexample.txt\n\n\n\n\n\n\nTipTHE SAME MODIFIER CAN MEAN DIFFERENT THINGS\n\n\n\n\n\nBoth grep and sed have the -i modifier, however in grep it means to perform case-insensitive matches, while in sed it means to perform the substitution in-place. It is very important to note that the same modifier can mean different things for different tools. It is always a good idea to check the help page for each tool. How will you perform a case-insensitive substitution in sed ?\n\n\n\nAnother useful command is tr or translate . While it is similar to sed in that it performs find+replace tasks, tr is faster and easier to use for character-by-character operations, while sed is better for longer strings. For example, if you want to replace a single character with another, tr can do it very easily. For example, lets pipe the sed output to tr to convert Directory into directory\nsed 's/dir/Directory/` list_of_files.txt |  tr 'D' 'd'\n\ndirectory1\ndirectory2\nexample.txt\nIn the above command, tr looks for all instances of D and converts it into d. While this can be done with sed, sed by default only performs operations for the FIRST INSTANCE PER LINE. In other words, sed splits your input line by line, and performs the operation independently for each line.\ntr performs the operation on a character by character basis, and therefore can work across different lines, as the newline (or \\n - which is the character that actually represents a new line) itself is treated as just another character. This allows you to do operations such as the one below very easily.\nsed 's/dir/Directory/` list_of_files.txt |  tr '\\n' '\\s'\n\nDirectory1 Directory2 example.txt\nHere, we replaced all newline characters (\\n) with spaces (\\s)\n\n\nWorking with bioinformatics data\nLets download some real bioinformatics data and see how we can use what we have learned so far.\n\n\n\n\n\n\nNote\n\n\n\nIf you have performed the setup.sh step in Module0 , you can skip to the exercise. The file should be present in NWU_2025_workshop_data/test_datasets/GCA_049744075.1 \n\n\nActivate your conda environment so that the necessary bioinformatics packages are loaded. See Module0: Setup for more information.\nconda activate NWU_2025_workshop\nDownload a genome fasta file using NCBI’s datasets tool. The datasets tool should already be installed in the conda environment. You can see how to use the datasets tool by typing datasets --help . To download a genome for a specific accession number, the command is :\ndatasets download genome accession GCA_049744075.1\nThis will download a zipped archive from NCBI. Lets extract the archive using unzip, take the file we need and then remove the rest.\nunzip ncbi_dataset.zip\nmkdir test_datasets\nmv ncbi_dataset/data/GCA_049744075.1/GCA_049744075.1_ASM4974407v1_genomic.fna test_datasets/\nrm -r ncbi_dataset.zip ncbi_dataset # you can keep specifying files/folders after the rm command to remove them all\n\n\nEXERCISE: Perform the following actions on the above fasta file\n\nCount the number of contigs in the fasta file\n\n\n\nClick to reveal answer\n\nThe grep command can be used to count the number of contigs (or sequences) in a FASTA file. Each sequence in a FASTA file starts with a header line (&gt;), so we can grep the &gt; character to get only the header lines, and add the -c flag to grep so that it reports the number of matches.\ngrep -c \"&gt;\" GCA_049744075.1_ASM4974407v1_genomic.fna\nAlternatively, we can also use wc -l to report the number of lines matched, as grep by default outputs all matching lines\ngrep \"&gt;\" GCA_049744075.1_ASM4974407v1_genomic.fna | wc -l\n\n\nExtract only sequence names from the FASTA File and write it to a new file.\n\n\n\nclick to reveal answer\n\nWe can extract only sequence names by grepping the “&gt;” character as done previously, but now we pipe it to sed to remove only the &gt; , keeping only the actual names themselves, and then redirect that output to a new text file.\ngrep \"&gt;\" GCA_049744075.1_ASM4974407v1_genomic.fna | sed 's/&gt;//' &gt; GCA_049744075.1_header_names.txt\n\n\nCount the total number of bases in the FASTA file.\n\n\n\nclick to reveal answer\n\nWe use inverted matching with grep -v to get all lines EXCEPT lines starting with “&gt;”, then we remove all newlines using tr -d, and then use wc to count the number of characters. We have to remove all newlines (\\n) as each \\n is considered a separate character, but here we only want to count the number of nucleotides.\ngrep -v \"&gt;\" GCA_049744075.1_ASM4974407v1_genomic.fna | tr -d '\\n' | wc -c\n\n\nIn silico restriction digestion - how many fragments will you get if you digest your genome with EcoRI?\n\n\n\nclick to reveal answer\n\nThe recognition site for EcoRI is GAATTC . We will only get the DNA sequence from our FASTA file using grep and tr as above, but now pipe it to sed and replace each occurrence of GAATTC with a newline (in effect, “cutting” our DNA), then we count the total number of lines\ngrep -v \"&gt;\" GCA_049744075.1_ASM4974407v1_genomic.fna | tr -d '\\n' | sed 's/GAATTC/\\n/g' | wc -l"
  },
  {
    "objectID": "module3.html",
    "href": "module3.html",
    "title": "Module 3",
    "section": "",
    "text": "Free, accessible, and global data can be integrated for comprehensive monitoring of infectious diseases\nHelps contextualize local outbreaks/epidemics on a global scale (i.e: Has this strain been seen before? When? Where?)\nAllows comparisons between different time periods, geographical locations, and strains of different genetic backgrounds.\n\nUnderstanding how to use the different sources of publicly available data along with careful interpretation of metadata is key for pathogen genomics research"
  },
  {
    "objectID": "module3.html#bioproject-biosample-and-sra",
    "href": "module3.html#bioproject-biosample-and-sra",
    "title": "Module 3",
    "section": "BioProject, BioSample, and SRA",
    "text": "BioProject, BioSample, and SRA\n\n\n\nNCBI tree\n\n\n\nBioProject\nThe BioProject serves as a central repository linking all deposited data for a specific research project. The BioProject is a single place that can be used to find all other diverse data types deposited under that one project.\nBioProjects will have identifiers (or accession number) starting with PRJNA\nThe BioProject accession for the source of the sample data we are using in this module is PRJNA1230142. Click on the link and take a look at the information available.\n\n\n\nBioProject details\n\n\n\n\n\nAvailable data under a single BioProject\n\n\nBelow which, you can also find individual links to each sample record under the BioProject along with the option to download the summary table.\n\n\nBioSample\nLike how the BioProject is the link to all samples deposited under a single project, the BioSample is the link to all data deposited for a single sample. This includes metadata for a single sample (organism, collection site, etc.), the raw reads for that sample (if deposited), and the genome assembly/annotation data. The BioSample also serves as link back to the BioProject.\nBioSamples will have identifiers (or accession number) starting with SAM\nClick on the BioSample link to any one of the records and take a look.\n\n\n\nBioSample details\n\n\nThe attributes section in each BioSample record comprises important metadata regarding when and where the sample was collected, and some information about strain background and genotype. Further below you can access the SRA (raw reads) and Nucleotide (assembly) records for each BioSample.\n\n\n\n\n\n\nTipThe challenge with publicly available metadata\n\n\n\n\n\nRemember that metadata is simply what is reported by the submitter. You will often find samples with vague descriptions, typos or even no metadata at all. When you are depositing samples in NCBI make sure to provide rich metadata with no errors!\n\n\n\n\n\nSRA (Sequence Read Archive): Raw fastq reads\nSRA is a repository of all high-throughput sequencing data. It contains raw sequence reads from genomic, metagenomic, and transcriptomic sources.\nThrough the BioSample, you can also access the raw reads by clicking on the SRA link at the bottom of the page. This will take you to the SRA experiment page.\nThe SRA experiment has information regarding how the reads were generated (i.e the experimental design). This also links back to the BioProject and BioSample.\nThe SRA experiment accessions start with SRX, ERX or DRX.\nThe SRA experiment page also link to the SRA run for that sample. The SRA run page has the actual .fastq sequence reads. The SRA runs have accessions that start with SRR,ERR or DRR\nThe SRA run page for a given sample also links back to the SRA experiment, BioSample and BioProject. It also has a download link to the raw fastq reads.\nNavigate to the SRR page for any sample and take a look at the information available.\n\n\n\nSRA Run - downloading reads\n\n\n\n\nNucleotide sequences\nNCBI has multiple nucleotide sequence databases, but all deposited sequences are stored in Nuccore - this can be individual contigs from each assembly, plasmid sequences, or gene sequences.\nThere are smaller databases with more specific nucleotide sequences branching from Nuccore, for eg: the Gene database for gene sequences and Genome database for whole genome sequences. The Genome (or ‘Assembly’) database accessions begin with ASM. The ASM record for a given sample provides the assembly itself, assembly statistics, annotation information and also links back to the BioSample and BioProject.\nThe assembly can be obtained from RefSeq or GenBank, two repositories of genome sequences deposited in NCBI with minor differences.For our purposes, we will mostly be interacting with GenBank. GenBank accessions start with GCA. Navigate to the genome assembly page for any sample and take a look.\n\n\n\nGenome assembly"
  },
  {
    "objectID": "module3.html#retrieving-data-from-ncbi-via-command-line",
    "href": "module3.html#retrieving-data-from-ncbi-via-command-line",
    "title": "Module 3",
    "section": "Retrieving Data from NCBI via Command Line",
    "text": "Retrieving Data from NCBI via Command Line\nAs you may have seen above, it is possible to click through the NCBI website and access various types of information from a given sample. However, this starts becoming impractical as soon as you need files on hundreds or even tens of samples. Luckily, there are useful command-line tools available that let us programmatically retrieve data from NCBI.\n\nncbi-datasets CLI\nThis is the official NCBI software for accessing different datasets. If you have the BioSample or BioProject accession, datasets lets you download assemblies, annotations and summary information.\ndatasets download genome accession GCA_000486855.2 --filename GCA_000486855.2.zip --include cds,genome,protein,gbff,gff3  \nA zip file will be downloaded which you can extract using the unzip command and access the files.\nunzip GCA_000486855.2.zip\nUse datasets download genome --help to see the different options, you can fetch a lot of data by using the right set of options!\n\n\nncbi-genome-download\nThis is an alternative to ncbi datasets that is very similar. ncbi-genome-download is not an official tool made and maintained by NCBI, it was just the bioinformatics community’s solution to downloading NCBI datasets before datasets existed. It is still a viable alternative today. It offers some options not found in datasets such as downloading assembly metadata in .tsv.\nncbi-genome-download -A GCA_049745\n255.1,GCA_049744095.1,GCA_049744075.1,GCA_049744695.1,GCA_049744875.1 -m metadata.tsv bacteria\n\n\nsra-tools\nThe SRA Toolkit is NCBI’s official software for downloading and extracting fastq files from SRA runs. It has two main parts - prefetch and fasterq-dump.\nprefetch downloads an SRA run in .sra format which can then be converted to .fastq using fasterq-dump. Alternatively you can also directly download reads using fasterq-dump\nfasterq-dump SRR3252850\n\nEXERCISE: retrieve data from NCBI using datasets\n\nDownload only the assemblies in the GenBank database from the BioProject PRJNA1230142\n\n\nClick to reveal answer\n\ndatasets download  genome accession PRJNA1230142 --assembly-source 'GenBank' --include 'genome' \n\nDownload all “chromosome” and “complete” level Salmonella enterica assemblies and their corresponding annotation files in genbank format from the GenBank database deposited after October 1st, 2025.\n\n\nClick to reveal answer\n\ndatasets download  genome taxon 'Salmonella enterica' --assembly-source 'GenBank' --assembly-level 'chromosome,complete' --include 'genome,gbff' --released-after 2025-10-01"
  }
]